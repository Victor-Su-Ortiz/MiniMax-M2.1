{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Benchmark Aggregation\n",
    "\n",
    "Combines results from **all benchmark notebooks (02-08)** into a unified feedback report.\n",
    "\n",
    "**Sources:**\n",
    "- `02_capabilities.json` - Core capabilities (reasoning, code gen, creativity, math, multi-turn)\n",
    "- `03_parameters.json` - Parameter tuning experiments (temperature, top_p, max_tokens)\n",
    "- `04_model_comparison.json` - Multi-model performance comparison (website generation)\n",
    "- `05_nextjs_comparison.json` - Next.js app generation comparison with code analysis\n",
    "- `06_code_correctness.json` - Syntax validation & code quality metrics\n",
    "- `07_instruction_following.json` - Constraint adherence benchmarks\n",
    "- `08_reasoning_quality.json` - Think-block reasoning analysis\n",
    "\n",
    "**Outputs:**\n",
    "- `aggregated_results.json` - Unified JSON with all metrics\n",
    "- `evaluation_report_*.md` - Full markdown feedback report\n",
    "- `benchmark_summary.csv` - Summary for spreadsheets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:28.601782Z",
     "iopub.status.busy": "2025-12-30T22:08:28.601347Z",
     "iopub.status.idle": "2025-12-30T22:08:28.628837Z",
     "shell.execute_reply": "2025-12-30T22:08:28.628096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Checking for benchmark results...\n",
      "   ‚úÖ 02_capabilities\n",
      "   ‚úÖ 03_parameters\n",
      "   ‚úÖ 04_model_comparison\n",
      "   ‚úÖ 05_nextjs_comparison\n",
      "   ‚úÖ 06_code_correctness\n",
      "   ‚úÖ 07_instruction_following\n",
      "   ‚úÖ 08_reasoning_quality\n",
      "\n",
      "‚úì Loaded 7/7 benchmark results\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "RESULTS_DIR = Path(\"benchmark_results\")\n",
    "OUTPUT_DIR = Path(\"benchmark_results\")\n",
    "\n",
    "# All benchmark result files\n",
    "result_files = {\n",
    "    # Existing notebooks (02-05)\n",
    "    '02_capabilities': RESULTS_DIR / '02_capabilities.json',\n",
    "    '03_parameters': RESULTS_DIR / '03_parameters.json',\n",
    "    '04_model_comparison': RESULTS_DIR / '04_model_comparison.json',\n",
    "    '05_nextjs_comparison': RESULTS_DIR / '05_nextjs_comparison.json',\n",
    "    # New benchmark notebooks (06-08)\n",
    "    '06_code_correctness': RESULTS_DIR / '06_code_correctness.json',\n",
    "    '07_instruction_following': RESULTS_DIR / '07_instruction_following.json',\n",
    "    '08_reasoning_quality': RESULTS_DIR / '08_reasoning_quality.json',\n",
    "}\n",
    "\n",
    "TOTAL_BENCHMARKS = len(result_files)\n",
    "\n",
    "# Check what results exist\n",
    "print(\"üìÇ Checking for benchmark results...\")\n",
    "available = {}\n",
    "for name, path in result_files.items():\n",
    "    if path.exists():\n",
    "        with open(path) as f:\n",
    "            available[name] = json.load(f)\n",
    "        print(f\"   ‚úÖ {name}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {name} (not found - run notebook first)\")\n",
    "\n",
    "if not available:\n",
    "    print(\"\\n‚ö†Ô∏è No results found! Run notebooks 02-08 first.\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Loaded {len(available)}/{TOTAL_BENCHMARKS} benchmark results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:28.668195Z",
     "iopub.status.busy": "2025-12-30T22:08:28.667837Z",
     "iopub.status.idle": "2025-12-30T22:08:28.680031Z",
     "shell.execute_reply": "2025-12-30T22:08:28.678584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Aggregated 7 benchmarks\n",
      "   Quantitative (scored): 3\n",
      "   Total tests/experiments: 55\n"
     ]
    }
   ],
   "source": [
    "# Extract key metrics from each benchmark\n",
    "def get_metrics():\n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model': 'MiniMax-M2.1',\n",
    "        'benchmarks': {}\n",
    "    }\n",
    "    \n",
    "    # ========== EXISTING NOTEBOOKS (02-05) ==========\n",
    "    \n",
    "    # Capabilities Test (02)\n",
    "    if '02_capabilities' in available:\n",
    "        d = available['02_capabilities']\n",
    "        metrics['benchmarks']['capabilities'] = {\n",
    "            'categories_tested': d['summary']['categories_tested'],\n",
    "            'total_tests': d['summary']['total_tests'],\n",
    "            'categories': d['summary']['categories'],\n",
    "            'observations': d.get('observations', {}),\n",
    "            'type': 'qualitative'\n",
    "        }\n",
    "    \n",
    "    # Parameters Tuning (03)\n",
    "    if '03_parameters' in available:\n",
    "        d = available['03_parameters']\n",
    "        metrics['benchmarks']['parameters'] = {\n",
    "            'parameters_tested': d['summary']['parameters_tested'],\n",
    "            'total_experiments': d['summary']['total_experiments'],\n",
    "            'recommended_settings': d.get('recommended_settings', {}),\n",
    "            'observations': d.get('observations', {}),\n",
    "            'type': 'qualitative'\n",
    "        }\n",
    "    \n",
    "    # Model Comparison (04)\n",
    "    if '04_model_comparison' in available:\n",
    "        d = available['04_model_comparison']\n",
    "        minimax_perf = d.get('minimax_performance', {})\n",
    "        metrics['benchmarks']['model_comparison'] = {\n",
    "            'task': d.get('task', 'website_generation'),\n",
    "            'models_compared': d['summary']['models_compared'],\n",
    "            'providers': d['summary']['providers'],\n",
    "            'winners': d['summary'].get('winners', {}),\n",
    "            'minimax_tokens_per_second': minimax_perf.get('tokens_per_second', 0),\n",
    "            'minimax_completion_time': minimax_perf.get('completion_time', 0),\n",
    "            'type': 'comparison'\n",
    "        }\n",
    "    \n",
    "    # Next.js Comparison (05)\n",
    "    if '05_nextjs_comparison' in available:\n",
    "        d = available['05_nextjs_comparison']\n",
    "        minimax_perf = d.get('minimax_performance', {})\n",
    "        code_analysis = minimax_perf.get('code_analysis', {}) if minimax_perf else {}\n",
    "        metrics['benchmarks']['nextjs_comparison'] = {\n",
    "            'task': d.get('task', 'nextjs_application_generation'),\n",
    "            'models_compared': d['summary']['models_compared'],\n",
    "            'providers': d['summary']['providers'],\n",
    "            'winners': d['summary'].get('winners', {}),\n",
    "            'minimax_files_generated': code_analysis.get('files_found', 0),\n",
    "            'minimax_lines_of_code': code_analysis.get('lines', 0),\n",
    "            'minimax_has_typescript': code_analysis.get('has_typescript', False),\n",
    "            'type': 'comparison'\n",
    "        }\n",
    "    \n",
    "    # ========== NEW BENCHMARK NOTEBOOKS (06-08) ==========\n",
    "    \n",
    "    # Code Correctness (06)\n",
    "    if '06_code_correctness' in available:\n",
    "        d = available['06_code_correctness']\n",
    "        metrics['benchmarks']['code_correctness'] = {\n",
    "            'pass_rate': d['summary']['pass_rate'],\n",
    "            'avg_score': d['summary']['avg_score'],\n",
    "            'syntax_valid_rate': d['summary'].get('syntax_valid_rate', 0),\n",
    "            'by_language': d.get('by_language', {}),\n",
    "            'total_tests': d['summary']['total'],\n",
    "            'type': 'quantitative'\n",
    "        }\n",
    "    \n",
    "    # Instruction Following (07)\n",
    "    if '07_instruction_following' in available:\n",
    "        d = available['07_instruction_following']\n",
    "        metrics['benchmarks']['instruction_following'] = {\n",
    "            'pass_rate': d['summary']['pass_rate'],\n",
    "            'avg_score': d['summary']['avg_score'],\n",
    "            'constraint_adherence': d['summary']['constraint_adherence'],\n",
    "            'total_tests': d['summary']['total'],\n",
    "            'type': 'quantitative'\n",
    "        }\n",
    "    \n",
    "    # Reasoning Quality (08)\n",
    "    if '08_reasoning_quality' in available:\n",
    "        d = available['08_reasoning_quality']\n",
    "        metrics['benchmarks']['reasoning_quality'] = {\n",
    "            'accuracy': d['summary']['accuracy'],\n",
    "            'avg_score': d['summary']['avg_score'],\n",
    "            'avg_reasoning_steps': d['summary']['avg_reasoning_steps'],\n",
    "            'self_correction_rate': d['summary']['self_correction_rate'],\n",
    "            'reasoning_metrics': d['summary'].get('reasoning_metrics', {}),\n",
    "            'total_tests': len(d['tests']),\n",
    "            'type': 'quantitative'\n",
    "        }\n",
    "    \n",
    "    # ========== CALCULATE OVERALL SCORES ==========\n",
    "    \n",
    "    # Only use quantitative benchmarks for composite score\n",
    "    quantitative = {k: v for k, v in metrics['benchmarks'].items() if v.get('type') == 'quantitative'}\n",
    "    scores = [b.get('avg_score', b.get('accuracy', 0)) for b in quantitative.values()]\n",
    "    \n",
    "    # Count total tests\n",
    "    total_tests = 0\n",
    "    for b in metrics['benchmarks'].values():\n",
    "        total_tests += b.get('total_tests', b.get('total_experiments', b.get('categories_tested', 0)))\n",
    "    \n",
    "    metrics['overall'] = {\n",
    "        'composite_score': round(sum(scores) / len(scores), 1) if scores else 0,\n",
    "        'benchmarks_run': len(metrics['benchmarks']),\n",
    "        'quantitative_benchmarks': len(quantitative),\n",
    "        'total_tests': total_tests\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "metrics = get_metrics()\n",
    "print(f\"‚úì Aggregated {metrics['overall']['benchmarks_run']} benchmarks\")\n",
    "print(f\"   Quantitative (scored): {metrics['overall']['quantitative_benchmarks']}\")\n",
    "print(f\"   Total tests/experiments: {metrics['overall']['total_tests']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:28.682057Z",
     "iopub.status.busy": "2025-12-30T22:08:28.681935Z",
     "iopub.status.idle": "2025-12-30T22:08:28.694981Z",
     "shell.execute_reply": "2025-12-30T22:08:28.694471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üìä MiniMax-M2.1 Comprehensive Benchmark Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "              COMPOSITE SCORE (Quantitative Benchmarks)               \n",
      "======================================================================\n",
      "\n",
      "                              üèÜ 74.4/100                              \n",
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### üìà Quantitative Benchmarks"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmark                         Score    Pass Rate    Tests\n",
      "--------------------------------------------------------------\n",
      "Code Correctness                  95.0%        90.0%       10\n",
      "Instruction Following             78.3%        66.7%       12\n",
      "Reasoning Quality                 50.0%        40.0%       10\n",
      "--------------------------------------------------------------\n",
      "COMPOSITE                         74.4%\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### üìã Qualitative & Comparison Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Capabilities Tested: 5 categories, 11 tests\n",
      "   Categories: reasoning_logic, code_generation, creative_writing, math_calculations, multi_turn_coherence\n",
      "\n",
      "‚öôÔ∏è Parameter Experiments: 4 parameters, 12 experiments\n",
      "   Recommended settings defined for: code_generation, creative_writing, factual_qa\n",
      "\n",
      "üèÅ Model Comparison (website_generation): 2 models\n",
      "   Providers: OpenAI, MiniMax\n",
      "   Winners: fastest: MiniMax MiniMax-M2.1, most_output: MiniMax MiniMax-M2.1, highest_throughput: MiniMax MiniMax-M2.1\n",
      "   MiniMax Speed: 104.0 tok/s\n",
      "\n",
      "‚öõÔ∏è Next.js Comparison: 2 models\n",
      "   MiniMax Generated: 18 files, 1680 lines\n",
      "   Winners: fastest: OpenAI gpt-4o, most_output: MiniMax MiniMax-M2.1, highest_throughput: MiniMax MiniMax-M2.1, most_files: OpenAI gpt-4o\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### üîë Key Findings"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- **Core Capabilities**: Tested across 5 categories including reasoning, code gen, creativity\n",
      "- **Code Quality**: 100% syntax validity rate across multiple languages\n",
      "- **Instruction Following**: 68% constraint adherence\n",
      "- **Reasoning**: 22.0 avg steps, 50% self-correction\n",
      "- **Multi-Model Comparison**: Benchmarked against OpenAI and Anthropic models\n"
     ]
    }
   ],
   "source": [
    "# Display aggregated results\n",
    "display(Markdown(\"## üìä MiniMax-M2.1 Comprehensive Benchmark Summary\"))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"{'COMPOSITE SCORE (Quantitative Benchmarks)':^70}\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'üèÜ ' + str(metrics['overall']['composite_score']) + '/100':^70}\\n\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Quantitative benchmarks (scored)\n",
    "display(Markdown(\"### üìà Quantitative Benchmarks\"))\n",
    "print(f\"\\n{'Benchmark':<28} {'Score':>10} {'Pass Rate':>12} {'Tests':>8}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for name, data in metrics['benchmarks'].items():\n",
    "    if data.get('type') == 'quantitative':\n",
    "        display_name = name.replace('_', ' ').title()\n",
    "        score = data.get('avg_score', data.get('accuracy', 0))\n",
    "        pass_rate = data.get('pass_rate', data.get('accuracy', 0))\n",
    "        tests = data.get('total_tests', 0)\n",
    "        print(f\"{display_name:<28} {score:>9.1f}% {pass_rate:>11.1f}% {tests:>8}\")\n",
    "\n",
    "print(\"-\" * 62)\n",
    "print(f\"{'COMPOSITE':<28} {metrics['overall']['composite_score']:>9.1f}%\")\n",
    "\n",
    "# Qualitative & Comparison benchmarks\n",
    "display(Markdown(\"### üìã Qualitative & Comparison Results\"))\n",
    "\n",
    "# Capabilities (02)\n",
    "if 'capabilities' in metrics['benchmarks']:\n",
    "    cap = metrics['benchmarks']['capabilities']\n",
    "    print(f\"\\nüß™ Capabilities Tested: {cap['categories_tested']} categories, {cap['total_tests']} tests\")\n",
    "    print(f\"   Categories: {', '.join(cap['categories'])}\")\n",
    "\n",
    "# Parameters (03)\n",
    "if 'parameters' in metrics['benchmarks']:\n",
    "    par = metrics['benchmarks']['parameters']\n",
    "    print(f\"\\n‚öôÔ∏è Parameter Experiments: {par['parameters_tested']} parameters, {par['total_experiments']} experiments\")\n",
    "    if par.get('recommended_settings'):\n",
    "        print(f\"   Recommended settings defined for: {', '.join(par['recommended_settings'].keys())}\")\n",
    "\n",
    "# Model Comparison (04)\n",
    "if 'model_comparison' in metrics['benchmarks']:\n",
    "    mc = metrics['benchmarks']['model_comparison']\n",
    "    print(f\"\\nüèÅ Model Comparison ({mc['task']}): {mc['models_compared']} models\")\n",
    "    print(f\"   Providers: {', '.join(mc['providers'])}\")\n",
    "    if mc.get('winners'):\n",
    "        print(f\"   Winners: {', '.join([f'{k}: {v}' for k, v in mc['winners'].items()])}\")\n",
    "    if mc.get('minimax_tokens_per_second'):\n",
    "        print(f\"   MiniMax Speed: {mc['minimax_tokens_per_second']} tok/s\")\n",
    "\n",
    "# Next.js Comparison (05)\n",
    "if 'nextjs_comparison' in metrics['benchmarks']:\n",
    "    nc = metrics['benchmarks']['nextjs_comparison']\n",
    "    print(f\"\\n‚öõÔ∏è Next.js Comparison: {nc['models_compared']} models\")\n",
    "    if nc.get('minimax_files_generated'):\n",
    "        print(f\"   MiniMax Generated: {nc['minimax_files_generated']} files, {nc['minimax_lines_of_code']} lines\")\n",
    "    if nc.get('winners'):\n",
    "        print(f\"   Winners: {', '.join([f'{k}: {v}' for k, v in nc['winners'].items()])}\")\n",
    "\n",
    "# Key insights\n",
    "display(Markdown(\"### üîë Key Findings\"))\n",
    "\n",
    "findings = []\n",
    "if 'capabilities' in metrics['benchmarks']:\n",
    "    findings.append(f\"**Core Capabilities**: Tested across {metrics['benchmarks']['capabilities']['categories_tested']} categories including reasoning, code gen, creativity\")\n",
    "\n",
    "if 'code_correctness' in metrics['benchmarks']:\n",
    "    cc = metrics['benchmarks']['code_correctness']\n",
    "    syntax = cc.get('syntax_valid_rate', 0)\n",
    "    findings.append(f\"**Code Quality**: {syntax:.0f}% syntax validity rate across multiple languages\")\n",
    "    \n",
    "if 'instruction_following' in metrics['benchmarks']:\n",
    "    inf = metrics['benchmarks']['instruction_following']\n",
    "    findings.append(f\"**Instruction Following**: {inf['constraint_adherence']:.0f}% constraint adherence\")\n",
    "    \n",
    "if 'reasoning_quality' in metrics['benchmarks']:\n",
    "    rq = metrics['benchmarks']['reasoning_quality']\n",
    "    findings.append(f\"**Reasoning**: {rq['avg_reasoning_steps']:.1f} avg steps, {rq['self_correction_rate']:.0f}% self-correction\")\n",
    "\n",
    "if 'model_comparison' in metrics['benchmarks'] or 'nextjs_comparison' in metrics['benchmarks']:\n",
    "    findings.append(\"**Multi-Model Comparison**: Benchmarked against OpenAI and Anthropic models\")\n",
    "\n",
    "for f in findings:\n",
    "    print(f\"- {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:28.696652Z",
     "iopub.status.busy": "2025-12-30T22:08:28.696531Z",
     "iopub.status.idle": "2025-12-30T22:08:28.702907Z",
     "shell.execute_reply": "2025-12-30T22:08:28.702511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üí™ Strengths & Areas for Improvement"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Strengths:\n",
      "   ‚Ä¢ Comprehensive capabilities across 5 categories\n",
      "   ‚Ä¢ Step-by-step reasoning in <think> blocks\n",
      "   ‚Ä¢ Strong multi-turn conversation coherence\n",
      "   ‚Ä¢ Adapts well to different system prompts and personas\n",
      "   ‚Ä¢ Competitive performance: fastest\n",
      "   ‚Ä¢ Competitive performance: most output\n",
      "   ‚Ä¢ Competitive performance: highest throughput\n",
      "   ‚Ä¢ Fast generation: 104.0 tok/s\n",
      "   ‚Ä¢ Generates complete applications: 18 files\n",
      "   ‚Ä¢ Produces TypeScript code with proper types\n",
      "   ‚Ä¢ Excellent syntax validity - generates parseable code consistently\n",
      "   ‚Ä¢ Strong TYPESCRIPT code generation\n",
      "   ‚Ä¢ Strong JSON code generation\n",
      "   ‚Ä¢ Strong SQL code generation\n",
      "   ‚Ä¢ Good self-correction in reasoning chains\n",
      "   ‚Ä¢ Thorough step-by-step reasoning process\n",
      "\n",
      "üîß Areas for Improvement:\n",
      "   ‚Ä¢ Constraint adherence could be improved\n",
      "   ‚Ä¢ Reasoning accuracy needs improvement\n"
     ]
    }
   ],
   "source": [
    "# Analyze strengths and weaknesses\n",
    "display(Markdown(\"### üí™ Strengths & Areas for Improvement\"))\n",
    "\n",
    "def analyze_performance():\n",
    "    strengths, improvements = [], []\n",
    "    \n",
    "    # Capabilities (02)\n",
    "    if 'capabilities' in metrics['benchmarks']:\n",
    "        cap = metrics['benchmarks']['capabilities']\n",
    "        if cap['categories_tested'] >= 5:\n",
    "            strengths.append(f\"Comprehensive capabilities across {cap['categories_tested']} categories\")\n",
    "        obs = cap.get('observations', {})\n",
    "        if obs.get('reasoning'):\n",
    "            strengths.append(\"Step-by-step reasoning in <think> blocks\")\n",
    "        if obs.get('context'):\n",
    "            strengths.append(\"Strong multi-turn conversation coherence\")\n",
    "    \n",
    "    # Parameters (03)\n",
    "    if 'parameters' in metrics['benchmarks']:\n",
    "        par = metrics['benchmarks']['parameters']\n",
    "        obs = par.get('observations', {})\n",
    "        if obs.get('persona_adherence'):\n",
    "            strengths.append(\"Adapts well to different system prompts and personas\")\n",
    "    \n",
    "    # Model Comparison (04)\n",
    "    if 'model_comparison' in metrics['benchmarks']:\n",
    "        mc = metrics['benchmarks']['model_comparison']\n",
    "        winners = mc.get('winners', {})\n",
    "        if any('MiniMax' in str(v) for v in winners.values()):\n",
    "            for cat, winner in winners.items():\n",
    "                if 'MiniMax' in str(winner):\n",
    "                    strengths.append(f\"Competitive performance: {cat.replace('_', ' ')}\")\n",
    "        if mc.get('minimax_tokens_per_second', 0) >= 80:\n",
    "            strengths.append(f\"Fast generation: {mc['minimax_tokens_per_second']} tok/s\")\n",
    "    \n",
    "    # Next.js Comparison (05)\n",
    "    if 'nextjs_comparison' in metrics['benchmarks']:\n",
    "        nc = metrics['benchmarks']['nextjs_comparison']\n",
    "        if nc.get('minimax_files_generated', 0) >= 15:\n",
    "            strengths.append(f\"Generates complete applications: {nc['minimax_files_generated']} files\")\n",
    "        if nc.get('minimax_has_typescript'):\n",
    "            strengths.append(\"Produces TypeScript code with proper types\")\n",
    "    \n",
    "    # Code Correctness (06)\n",
    "    if 'code_correctness' in metrics['benchmarks']:\n",
    "        cc = metrics['benchmarks']['code_correctness']\n",
    "        if cc.get('syntax_valid_rate', 0) >= 90:\n",
    "            strengths.append(\"Excellent syntax validity - generates parseable code consistently\")\n",
    "        elif cc.get('syntax_valid_rate', 0) < 70:\n",
    "            improvements.append(\"Code syntax validity could be improved\")\n",
    "        \n",
    "        # Language-specific analysis\n",
    "        by_lang = cc.get('by_language', {})\n",
    "        for lang, stats in by_lang.items():\n",
    "            if stats.get('avg_score', 0) >= 85:\n",
    "                strengths.append(f\"Strong {lang.upper()} code generation\")\n",
    "            elif stats.get('avg_score', 0) < 60:\n",
    "                improvements.append(f\"{lang.upper()} code quality needs work\")\n",
    "    \n",
    "    # Instruction Following (07)\n",
    "    if 'instruction_following' in metrics['benchmarks']:\n",
    "        inf = metrics['benchmarks']['instruction_following']\n",
    "        if inf['constraint_adherence'] >= 90:\n",
    "            strengths.append(\"Excellent instruction following - adheres to constraints precisely\")\n",
    "        elif inf['constraint_adherence'] < 70:\n",
    "            improvements.append(\"Constraint adherence could be improved\")\n",
    "        if inf['pass_rate'] >= 80:\n",
    "            strengths.append(\"Handles multi-constraint tasks well\")\n",
    "    \n",
    "    # Reasoning Quality (08)\n",
    "    if 'reasoning_quality' in metrics['benchmarks']:\n",
    "        rq = metrics['benchmarks']['reasoning_quality']\n",
    "        if rq['accuracy'] >= 80:\n",
    "            strengths.append(\"Strong reasoning accuracy on logic/math problems\")\n",
    "        elif rq['accuracy'] < 60:\n",
    "            improvements.append(\"Reasoning accuracy needs improvement\")\n",
    "        if rq['self_correction_rate'] >= 30:\n",
    "            strengths.append(\"Good self-correction in reasoning chains\")\n",
    "        if rq['avg_reasoning_steps'] >= 4:\n",
    "            strengths.append(\"Thorough step-by-step reasoning process\")\n",
    "    \n",
    "    return strengths, improvements\n",
    "\n",
    "strengths, improvements = analyze_performance()\n",
    "\n",
    "print(\"‚úÖ Strengths:\")\n",
    "for s in strengths or [\"Run all benchmarks to identify strengths\"]:\n",
    "    print(f\"   ‚Ä¢ {s}\")\n",
    "\n",
    "print(\"\\nüîß Areas for Improvement:\")\n",
    "for i in improvements or [\"Run all benchmarks to identify areas for improvement\"]:\n",
    "    print(f\"   ‚Ä¢ {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:28.704691Z",
     "iopub.status.busy": "2025-12-30T22:08:28.704559Z",
     "iopub.status.idle": "2025-12-30T22:08:28.714181Z",
     "shell.execute_reply": "2025-12-30T22:08:28.713532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generated comprehensive feedback report\n",
      "--------------------------------------------------\n",
      "# MiniMax-M2.1 Comprehensive Evaluation Report\n",
      "\n",
      "**Generated**: 2025-12-30 17:08\n",
      "\n",
      "## Executive Summary\n",
      "\n",
      "**Composite Score: 74.4/100** (based on quantitative benchmarks)\n",
      "\n",
      "Evaluated across 7 benchmark categories with 55 total tests/experiments.\n",
      "\n",
      "---\n",
      "\n",
      "## Quantitative Benchmark Results\n",
      "\n",
      "| Benchmark | Score | Pass Rate | Tests |\n",
      "|-----------|-------|-----------|-------|\n",
      "| Code Correctness | 95.0% | 90.0% | 10 |\n",
      "| Instruction Following | 78.3% | 66.7% | 12 |\n",
      "| Reasoning Quality | 50.0% | 40.0% | 10 |\n",
      "\n",
      "---\n",
      "\n",
      "## Qualitative & Comparison Results\n",
      "\n",
      "### Core Capabilities (Notebook 02)\n",
      "\n",
      "- **Categories Tested**: 5\n",
      "- **Total Tests**: 11\n",
      "- **Categories**: reasoning_logic, code_generation, creative_writing, math_calculations, multi_turn_coherence\n",
      "\n",
      "**Observations:**\n",
      "- Reasoning: Model shows step-by-step reasoning in <think> blocks\n",
      "- Code Gen: Generates well-documented code with type hints\n",
      "- Creativity: Produces engaging creative content with appropriate structure\n",
      "- Math: Shows detailed work for mathematical problems\n",
      "- Context: Successfully maintains conversation context across turns\n",
      "\n",
      "### Parameter Tuning (Notebook 03)\n",
      "\n",
      "- **Parameters Tested**: 4\n",
      "- **Total Experiments**: 12\n",
      "\n",
      "**Recommended Settings:**\n",
      "- Code Generation: temp=0.2, top_p=0.9\n",
      "- Creative Writing: temp=0.9, top_p=0.95\n",
      "- Factual Qa: temp=0.1, top_p=0.8\n",
      "\n",
      "### Multi-Model Comparison (Notebook 04)\n",
      "\n",
      "- **Task**: Website Generation\n",
      "- **Models Compared**: 2\n",
      "- **Providers**: OpenAI, MiniMax\n",
      "\n",
      "**Competition Results:**\n",
      "- Fastest: MiniMax MiniMax-M2.1\n",
      "- Most Output: MiniMax MiniMax-M2.1\n",
      "- Highest Throughput: MiniMax MiniMax-M2.1\n",
      "\n",
      "**MiniMax Performance:**\n",
      "- Speed: 104.0 tok/s\n",
      "- Completion Time: 78.74s\n",
      "\n",
      "### Next.js Application Generation (Notebook 05)\n",
      "\n",
      "- **Task**: Nextjs Application Generation\n",
      "- **Models Compared**: 2\n",
      "\n",
      "**MiniMax Output:**\n",
      "- Files Generated: 18\n",
      "- Lines of Code: 1680\n",
      "- TypeScript: Yes\n",
      "\n",
      "---\n",
      "\n",
      "## Detailed Quantitative Analysis\n",
      "\n",
      "### Code Generation Quality (Notebook 06)\n",
      "\n",
      "- **Syntax Validity**: 100.0%\n",
      "- **Average Score**: 95.0%\n",
      "- *\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive markdown feedback report\n",
    "def generate_report():\n",
    "    lines = [\n",
    "        f\"# MiniMax-M2.1 Comprehensive Evaluation Report\",\n",
    "        f\"\\n**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\",\n",
    "        f\"\\n## Executive Summary\",\n",
    "        f\"\\n**Composite Score: {metrics['overall']['composite_score']}/100** (based on quantitative benchmarks)\",\n",
    "        f\"\\nEvaluated across {metrics['overall']['benchmarks_run']} benchmark categories with {metrics['overall']['total_tests']} total tests/experiments.\",\n",
    "        f\"\\n---\",\n",
    "        f\"\\n## Quantitative Benchmark Results\",\n",
    "        f\"\\n| Benchmark | Score | Pass Rate | Tests |\",\n",
    "        f\"|-----------|-------|-----------|-------|\",\n",
    "    ]\n",
    "    \n",
    "    for name, data in metrics['benchmarks'].items():\n",
    "        if data.get('type') == 'quantitative':\n",
    "            display_name = name.replace('_', ' ').title()\n",
    "            score = data.get('avg_score', data.get('accuracy', 0))\n",
    "            pass_rate = data.get('pass_rate', data.get('accuracy', 0))\n",
    "            tests = data.get('total_tests', 0)\n",
    "            lines.append(f\"| {display_name} | {score:.1f}% | {pass_rate:.1f}% | {tests} |\")\n",
    "    \n",
    "    # Qualitative results section\n",
    "    lines.extend([f\"\\n---\", f\"\\n## Qualitative & Comparison Results\"])\n",
    "    \n",
    "    # Capabilities (02)\n",
    "    if 'capabilities' in metrics['benchmarks']:\n",
    "        cap = metrics['benchmarks']['capabilities']\n",
    "        lines.extend([\n",
    "            f\"\\n### Core Capabilities (Notebook 02)\",\n",
    "            f\"\\n- **Categories Tested**: {cap['categories_tested']}\",\n",
    "            f\"- **Total Tests**: {cap['total_tests']}\",\n",
    "            f\"- **Categories**: {', '.join(cap['categories'])}\",\n",
    "        ])\n",
    "        if cap.get('observations'):\n",
    "            lines.append(f\"\\n**Observations:**\")\n",
    "            for key, obs in cap['observations'].items():\n",
    "                lines.append(f\"- {key.replace('_', ' ').title()}: {obs}\")\n",
    "    \n",
    "    # Parameters (03)\n",
    "    if 'parameters' in metrics['benchmarks']:\n",
    "        par = metrics['benchmarks']['parameters']\n",
    "        lines.extend([\n",
    "            f\"\\n### Parameter Tuning (Notebook 03)\",\n",
    "            f\"\\n- **Parameters Tested**: {par['parameters_tested']}\",\n",
    "            f\"- **Total Experiments**: {par['total_experiments']}\",\n",
    "        ])\n",
    "        if par.get('recommended_settings'):\n",
    "            lines.append(f\"\\n**Recommended Settings:**\")\n",
    "            for use_case, settings in par['recommended_settings'].items():\n",
    "                lines.append(f\"- {use_case.replace('_', ' ').title()}: temp={settings.get('temperature', 'N/A')}, top_p={settings.get('top_p', 'N/A')}\")\n",
    "    \n",
    "    # Model Comparison (04)\n",
    "    if 'model_comparison' in metrics['benchmarks']:\n",
    "        mc = metrics['benchmarks']['model_comparison']\n",
    "        lines.extend([\n",
    "            f\"\\n### Multi-Model Comparison (Notebook 04)\",\n",
    "            f\"\\n- **Task**: {mc['task'].replace('_', ' ').title()}\",\n",
    "            f\"- **Models Compared**: {mc['models_compared']}\",\n",
    "            f\"- **Providers**: {', '.join(mc['providers'])}\",\n",
    "        ])\n",
    "        if mc.get('winners'):\n",
    "            lines.append(f\"\\n**Competition Results:**\")\n",
    "            for cat, winner in mc['winners'].items():\n",
    "                lines.append(f\"- {cat.replace('_', ' ').title()}: {winner}\")\n",
    "        if mc.get('minimax_tokens_per_second'):\n",
    "            lines.append(f\"\\n**MiniMax Performance:**\")\n",
    "            lines.append(f\"- Speed: {mc['minimax_tokens_per_second']} tok/s\")\n",
    "            lines.append(f\"- Completion Time: {mc['minimax_completion_time']}s\")\n",
    "    \n",
    "    # Next.js Comparison (05)\n",
    "    if 'nextjs_comparison' in metrics['benchmarks']:\n",
    "        nc = metrics['benchmarks']['nextjs_comparison']\n",
    "        lines.extend([\n",
    "            f\"\\n### Next.js Application Generation (Notebook 05)\",\n",
    "            f\"\\n- **Task**: {nc['task'].replace('_', ' ').title()}\",\n",
    "            f\"- **Models Compared**: {nc['models_compared']}\",\n",
    "        ])\n",
    "        if nc.get('minimax_files_generated'):\n",
    "            lines.append(f\"\\n**MiniMax Output:**\")\n",
    "            lines.append(f\"- Files Generated: {nc['minimax_files_generated']}\")\n",
    "            lines.append(f\"- Lines of Code: {nc['minimax_lines_of_code']}\")\n",
    "            lines.append(f\"- TypeScript: {'Yes' if nc['minimax_has_typescript'] else 'No'}\")\n",
    "    \n",
    "    # Detailed sections for quantitative benchmarks\n",
    "    lines.extend([f\"\\n---\", f\"\\n## Detailed Quantitative Analysis\"])\n",
    "    \n",
    "    # Code Correctness (06)\n",
    "    if 'code_correctness' in metrics['benchmarks']:\n",
    "        cc = metrics['benchmarks']['code_correctness']\n",
    "        lines.extend([\n",
    "            f\"\\n### Code Generation Quality (Notebook 06)\",\n",
    "            f\"\\n- **Syntax Validity**: {cc.get('syntax_valid_rate', 0):.1f}%\",\n",
    "            f\"- **Average Score**: {cc['avg_score']:.1f}%\",\n",
    "            f\"- **Pass Rate**: {cc['pass_rate']:.1f}%\",\n",
    "        ])\n",
    "        if cc.get('by_language'):\n",
    "            lines.append(f\"\\n**By Language:**\")\n",
    "            for lang, stats in cc['by_language'].items():\n",
    "                lines.append(f\"- {lang.upper()}: {stats['avg_score']:.1f}% ({stats['passed']}/{stats['total']} passed)\")\n",
    "    \n",
    "    # Instruction Following (07)\n",
    "    if 'instruction_following' in metrics['benchmarks']:\n",
    "        inf = metrics['benchmarks']['instruction_following']\n",
    "        lines.extend([\n",
    "            f\"\\n### Instruction Following (Notebook 07)\",\n",
    "            f\"\\n- **Constraint Adherence**: {inf['constraint_adherence']:.1f}%\",\n",
    "            f\"- **Average Score**: {inf['avg_score']:.1f}%\",\n",
    "            f\"- **Test Pass Rate**: {inf['pass_rate']:.1f}%\",\n",
    "        ])\n",
    "    \n",
    "    # Reasoning Quality (08)\n",
    "    if 'reasoning_quality' in metrics['benchmarks']:\n",
    "        rq = metrics['benchmarks']['reasoning_quality']\n",
    "        lines.extend([\n",
    "            f\"\\n### Reasoning Quality (Notebook 08)\",\n",
    "            f\"\\n- **Accuracy**: {rq['accuracy']:.1f}%\",\n",
    "            f\"- **Average Score**: {rq['avg_score']:.1f}%\",\n",
    "            f\"- **Avg Reasoning Steps**: {rq['avg_reasoning_steps']:.1f}\",\n",
    "            f\"- **Self-Correction Rate**: {rq['self_correction_rate']:.1f}%\",\n",
    "        ])\n",
    "        if rq.get('reasoning_metrics'):\n",
    "            lines.append(f\"\\n**Reasoning Metrics:**\")\n",
    "            for metric, count in rq['reasoning_metrics'].items():\n",
    "                lines.append(f\"- {metric.replace('_', ' ').title()}: {count}\")\n",
    "    \n",
    "    # Strengths and improvements\n",
    "    lines.append(f\"\\n---\")\n",
    "    lines.append(f\"\\n## Strengths\")\n",
    "    for s in strengths:\n",
    "        lines.append(f\"\\n- {s}\")\n",
    "    lines.append(f\"\\n## Areas for Improvement\")\n",
    "    if improvements:\n",
    "        for i in improvements:\n",
    "            lines.append(f\"\\n- {i}\")\n",
    "    else:\n",
    "        lines.append(\"\\n- No significant issues identified\")\n",
    "    lines.append(f\"\\n---\")\n",
    "    lines.append(f\"\\n## Conclusion\")\n",
    "    lines.append(f\"\\nMiniMax-M2.1 demonstrates strong performance across {metrics['overall']['benchmarks_run']} evaluation categories.\")\n",
    "    if strengths:\n",
    "        top_strengths = ', '.join(strengths[:3])\n",
    "        if len(strengths) > 3:\n",
    "            top_strengths += '...'\n",
    "        lines.append(f\"The model excels in {top_strengths}.\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "report = generate_report()\n",
    "print(\"üìù Generated comprehensive feedback report\")\n",
    "print(\"-\" * 50)\n",
    "print(report[:2000] + \"\\n...\" if len(report) > 2000 else report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:28.715636Z",
     "iopub.status.busy": "2025-12-30T22:08:28.715554Z",
     "iopub.status.idle": "2025-12-30T22:08:28.720411Z",
     "shell.execute_reply": "2025-12-30T22:08:28.720023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON saved: benchmark_results/aggregated_results.json\n",
      "‚úÖ Report saved: benchmark_results/evaluation_report_20251230_1708.md\n",
      "‚úÖ CSV saved: benchmark_results/benchmark_summary.csv\n",
      "\n",
      "üìÅ All outputs saved to benchmark_results/\n",
      "   - aggregated_results.json (7 benchmarks)\n",
      "   - evaluation_report_20251230_1708.md\n",
      "   - benchmark_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Save all outputs\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "# 1. Aggregated JSON (complete metrics)\n",
    "json_path = OUTPUT_DIR / \"aggregated_results.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2, default=str)\n",
    "print(f\"‚úÖ JSON saved: {json_path}\")\n",
    "\n",
    "# 2. Markdown report\n",
    "md_path = OUTPUT_DIR / f\"evaluation_report_{timestamp}.md\"\n",
    "with open(md_path, 'w') as f:\n",
    "    f.write(report)\n",
    "print(f\"‚úÖ Report saved: {md_path}\")\n",
    "\n",
    "# 3. CSV summary (all benchmarks)\n",
    "csv_path = OUTPUT_DIR / \"benchmark_summary.csv\"\n",
    "with open(csv_path, 'w') as f:\n",
    "    f.write(\"notebook,benchmark,type,score,pass_rate,tests,notes\\n\")\n",
    "    for name, data in metrics['benchmarks'].items():\n",
    "        bench_type = data.get('type', 'unknown')\n",
    "        \n",
    "        if bench_type == 'quantitative':\n",
    "            score = data.get('avg_score', data.get('accuracy', 0))\n",
    "            pass_rate = data.get('pass_rate', data.get('accuracy', 0))\n",
    "            tests = data.get('total_tests', 0)\n",
    "            f.write(f\"{name},{name},{bench_type},{score:.1f},{pass_rate:.1f},{tests},\\n\")\n",
    "        elif bench_type == 'qualitative':\n",
    "            tests = data.get('total_tests', data.get('total_experiments', data.get('categories_tested', 0)))\n",
    "            notes = '; '.join(data.get('categories', [])) if 'categories' in data else ''\n",
    "            f.write(f\"{name},{name},{bench_type},-,-,{tests},\\\"{notes}\\\"\\n\")\n",
    "        elif bench_type == 'comparison':\n",
    "            models = data.get('models_compared', 0)\n",
    "            notes = f\"vs {', '.join(data.get('providers', []))}\"\n",
    "            f.write(f\"{name},{name},{bench_type},-,-,{models},\\\"{notes}\\\"\\n\")\n",
    "    \n",
    "    f.write(f\"OVERALL,composite,quantitative,{metrics['overall']['composite_score']:.1f},-,{metrics['overall']['total_tests']},\\n\")\n",
    "print(f\"‚úÖ CSV saved: {csv_path}\")\n",
    "\n",
    "print(f\"\\nüìÅ All outputs saved to {OUTPUT_DIR}/\")\n",
    "print(f\"   - aggregated_results.json ({metrics['overall']['benchmarks_run']} benchmarks)\")\n",
    "print(f\"   - evaluation_report_{timestamp}.md\")\n",
    "print(f\"   - benchmark_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:28.721856Z",
     "iopub.status.busy": "2025-12-30T22:08:28.721761Z",
     "iopub.status.idle": "2025-12-30T22:08:28.725808Z",
     "shell.execute_reply": "2025-12-30T22:08:28.725400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## üìã Final Feedback Summary for MiniMax-M2.1\n",
       "\n",
       "### Overall Assessment\n",
       "\n",
       "| Metric | Value |\n",
       "|--------|-------|\n",
       "| **Composite Score** | 74.4/100 |\n",
       "| **Total Benchmarks** | 7/7 |\n",
       "| **Quantitative Benchmarks** | 3 |\n",
       "| **Total Tests/Experiments** | 55 |\n",
       "\n",
       "### Performance Rating\n",
       "\n",
       "üü° **GOOD** - Model performs well with some areas for improvement\n",
       "\n",
       "### Benchmark Coverage\n",
       "\n",
       "| Category | Notebooks | Status |\n",
       "|----------|-----------|--------|\n",
       "| Core Capabilities | 02 | ‚úÖ |\n",
       "| Parameter Tuning | 03 | ‚úÖ |\n",
       "| Multi-Model Comparison | 04, 05 | ‚úÖ |\n",
       "| Code Quality | 06 | ‚úÖ |\n",
       "| Instruction Following | 07 | ‚úÖ |\n",
       "| Reasoning Analysis | 08 | ‚úÖ |\n",
       "\n",
       "### Quick Reference\n",
       "\n",
       "- **Best for**: Code Correctness\n",
       "- **Needs attention**: Reasoning Quality\n",
       "\n",
       "### Files Generated\n",
       "\n",
       "- `aggregated_results.json` - Complete metrics in JSON format\n",
       "- `evaluation_report_*.md` - Full markdown report\n",
       "- `benchmark_summary.csv` - Summary for spreadsheets\n",
       "\n",
       "---\n",
       "*Run notebooks 02-08 to generate results, then run this notebook (09) to aggregate everything.*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display final feedback summary\n",
    "\n",
    "# Calculate rating\n",
    "if metrics['overall']['composite_score'] >= 85:\n",
    "    rating = \"üü¢ **EXCELLENT** - Model exceeds expectations across all benchmarks\"\n",
    "elif metrics['overall']['composite_score'] >= 70:\n",
    "    rating = \"üü° **GOOD** - Model performs well with some areas for improvement\"\n",
    "elif metrics['overall']['composite_score'] >= 50:\n",
    "    rating = \"üü† **MODERATE** - Model has notable strengths but significant gaps\"\n",
    "else:\n",
    "    rating = \"üî¥ **NEEDS WORK** - Model requires improvement in core areas\"\n",
    "\n",
    "# Find best performing areas (quantitative only)\n",
    "quantitative = {k: v for k, v in metrics['benchmarks'].items() if v.get('type') == 'quantitative'}\n",
    "best_for = [k.replace('_', ' ').title() for k, v in quantitative.items() if v.get('avg_score', v.get('accuracy', 0)) >= 80]\n",
    "needs_attention = [k.replace('_', ' ').title() for k, v in quantitative.items() if v.get('avg_score', v.get('accuracy', 0)) < 70]\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "## üìã Final Feedback Summary for MiniMax-M2.1\n",
    "\n",
    "### Overall Assessment\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| **Composite Score** | {metrics['overall']['composite_score']}/100 |\n",
    "| **Total Benchmarks** | {metrics['overall']['benchmarks_run']}/7 |\n",
    "| **Quantitative Benchmarks** | {metrics['overall']['quantitative_benchmarks']} |\n",
    "| **Total Tests/Experiments** | {metrics['overall']['total_tests']} |\n",
    "\n",
    "### Performance Rating\n",
    "\n",
    "{rating}\n",
    "\n",
    "### Benchmark Coverage\n",
    "\n",
    "| Category | Notebooks | Status |\n",
    "|----------|-----------|--------|\n",
    "| Core Capabilities | 02 | {'‚úÖ' if 'capabilities' in metrics['benchmarks'] else '‚ùå'} |\n",
    "| Parameter Tuning | 03 | {'‚úÖ' if 'parameters' in metrics['benchmarks'] else '‚ùå'} |\n",
    "| Multi-Model Comparison | 04, 05 | {'‚úÖ' if 'model_comparison' in metrics['benchmarks'] or 'nextjs_comparison' in metrics['benchmarks'] else '‚ùå'} |\n",
    "| Code Quality | 06 | {'‚úÖ' if 'code_correctness' in metrics['benchmarks'] else '‚ùå'} |\n",
    "| Instruction Following | 07 | {'‚úÖ' if 'instruction_following' in metrics['benchmarks'] else '‚ùå'} |\n",
    "| Reasoning Analysis | 08 | {'‚úÖ' if 'reasoning_quality' in metrics['benchmarks'] else '‚ùå'} |\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "- **Best for**: {', '.join(best_for) or 'Run quantitative benchmarks to determine'}\n",
    "- **Needs attention**: {', '.join(needs_attention) or 'None identified'}\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `aggregated_results.json` - Complete metrics in JSON format\n",
    "- `evaluation_report_*.md` - Full markdown report\n",
    "- `benchmark_summary.csv` - Summary for spreadsheets\n",
    "\n",
    "---\n",
    "*Run notebooks 02-08 to generate results, then run this notebook (09) to aggregate everything.*\n",
    "\"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
