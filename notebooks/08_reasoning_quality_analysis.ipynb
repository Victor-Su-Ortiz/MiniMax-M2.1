{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning Quality Analysis\n",
    "\n",
    "Analyzes MiniMax's `<think>` blocks to quantify reasoning quality - a unique differentiator.\n",
    "\n",
    "**Metrics:**\n",
    "1. Reasoning depth (step count in think blocks)\n",
    "2. Self-correction detection (catches own errors?)\n",
    "3. Multi-path exploration (considers alternatives?)\n",
    "4. Edge case consideration\n",
    "5. Logic puzzle performance\n",
    "6. Mathematical reasoning accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:05:46.189288Z",
     "iopub.status.busy": "2025-12-30T22:05:46.188956Z",
     "iopub.status.idle": "2025-12-30T22:05:46.459651Z",
     "shell.execute_reply": "2025-12-30T22:05:46.459149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete | Model: MiniMax-M2.1\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys, os, time, json, re\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "from src.minimax_client import MiniMaxClient\n",
    "\n",
    "@dataclass\n",
    "class ReasoningMetrics:\n",
    "    thinking_length: int = 0\n",
    "    step_count: int = 0\n",
    "    has_self_correction: bool = False\n",
    "    considers_alternatives: bool = False\n",
    "    mentions_edge_cases: bool = False\n",
    "    shows_verification: bool = False\n",
    "\n",
    "@dataclass\n",
    "class TestResult:\n",
    "    name: str\n",
    "    passed: bool\n",
    "    score: float\n",
    "    answer_correct: bool\n",
    "    reasoning: ReasoningMetrics = None\n",
    "    thinking_content: str = \"\"\n",
    "    final_answer: str = \"\"\n",
    "    expected_answer: str = \"\"\n",
    "    completion_time: float = 0.0\n",
    "    tokens_used: int = 0\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResults:\n",
    "    notebook: str\n",
    "    timestamp: str\n",
    "    tests: list = field(default_factory=list)\n",
    "    \n",
    "    @property\n",
    "    def pass_rate(self): return sum(1 for t in self.tests if t.passed) / len(self.tests) * 100 if self.tests else 0\n",
    "    @property\n",
    "    def avg_score(self): return sum(t.score for t in self.tests) / len(self.tests) if self.tests else 0\n",
    "    @property\n",
    "    def avg_reasoning_steps(self): \n",
    "        steps = [t.reasoning.step_count for t in self.tests if t.reasoning]\n",
    "        return sum(steps) / len(steps) if steps else 0\n",
    "    @property\n",
    "    def self_correction_rate(self):\n",
    "        relevant = [t for t in self.tests if t.reasoning]\n",
    "        return sum(1 for t in relevant if t.reasoning.has_self_correction) / len(relevant) * 100 if relevant else 0\n",
    "\n",
    "client = MiniMaxClient()\n",
    "print(f\"‚úì Setup complete | Model: {client.model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:05:46.473873Z",
     "iopub.status.busy": "2025-12-30T22:05:46.473772Z",
     "iopub.status.idle": "2025-12-30T22:05:46.477488Z",
     "shell.execute_reply": "2025-12-30T22:05:46.477108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "# Reasoning analysis functions\n",
    "def extract_thinking(response: str) -> str:\n",
    "    \"\"\"Extract content from <think> blocks.\"\"\"\n",
    "    match = re.search(r'<think>(.*?)</think>', response, re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "def extract_answer(response: str) -> str:\n",
    "    \"\"\"Extract final answer (after thinking).\"\"\"\n",
    "    return re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL).strip()\n",
    "\n",
    "def analyze_reasoning(thinking: str) -> ReasoningMetrics:\n",
    "    \"\"\"Analyze the quality of reasoning in think block.\"\"\"\n",
    "    if not thinking:\n",
    "        return ReasoningMetrics()\n",
    "    \n",
    "    lower = thinking.lower()\n",
    "    \n",
    "    # Count reasoning steps (look for numbered steps, \"first\", \"then\", \"so\", etc.)\n",
    "    step_markers = len(re.findall(r'\\b(first|second|third|then|next|so|therefore|thus|hence|step \\d)\\b', lower))\n",
    "    sentence_count = len(re.findall(r'[.!?]+', thinking))\n",
    "    step_count = max(step_markers, sentence_count // 3)  # Rough estimate\n",
    "    \n",
    "    # Self-correction detection\n",
    "    correction_patterns = [r'wait', r'no,', r'actually', r'let me reconsider', r'i made a mistake', \n",
    "                          r'that\\'s wrong', r'correction', r'oops', r'hold on']\n",
    "    has_correction = any(re.search(p, lower) for p in correction_patterns)\n",
    "    \n",
    "    # Alternative exploration\n",
    "    alt_patterns = [r'alternatively', r'another way', r'could also', r'or we could', r'option \\d', \n",
    "                   r'approach \\d', r'method \\d', r'let\\'s try']\n",
    "    considers_alts = any(re.search(p, lower) for p in alt_patterns)\n",
    "    \n",
    "    # Edge case consideration\n",
    "    edge_patterns = [r'edge case', r'corner case', r'what if', r'special case', r'boundary', \n",
    "                    r'empty', r'null', r'zero', r'negative', r'overflow']\n",
    "    mentions_edges = any(re.search(p, lower) for p in edge_patterns)\n",
    "    \n",
    "    # Verification/checking\n",
    "    verify_patterns = [r'let me check', r'verify', r'double check', r'to confirm', r'checking', \n",
    "                      r'let\\'s see if', r'does this work']\n",
    "    shows_verify = any(re.search(p, lower) for p in verify_patterns)\n",
    "    \n",
    "    return ReasoningMetrics(\n",
    "        thinking_length=len(thinking),\n",
    "        step_count=step_count,\n",
    "        has_self_correction=has_correction,\n",
    "        considers_alternatives=considers_alts,\n",
    "        mentions_edge_cases=mentions_edges,\n",
    "        shows_verification=shows_verify\n",
    "    )\n",
    "\n",
    "def check_answer(answer: str, expected: str, answer_type: str = \"exact\") -> bool:\n",
    "    \"\"\"Check if answer matches expected.\"\"\"\n",
    "    answer_clean = answer.lower().strip()\n",
    "    expected_clean = expected.lower().strip()\n",
    "    \n",
    "    if answer_type == \"contains\":\n",
    "        return expected_clean in answer_clean\n",
    "    elif answer_type == \"number\":\n",
    "        # Extract numbers from both\n",
    "        ans_nums = re.findall(r'-?\\d+\\.?\\d*', answer_clean)\n",
    "        exp_nums = re.findall(r'-?\\d+\\.?\\d*', expected_clean)\n",
    "        return bool(ans_nums) and bool(exp_nums) and float(ans_nums[0]) == float(exp_nums[0])\n",
    "    else:\n",
    "        return expected_clean in answer_clean\n",
    "\n",
    "print(\"‚úì Analysis functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:05:46.478544Z",
     "iopub.status.busy": "2025-12-30T22:05:46.478481Z",
     "iopub.status.idle": "2025-12-30T22:05:46.481025Z",
     "shell.execute_reply": "2025-12-30T22:05:46.480631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì 10 reasoning tests defined\n"
     ]
    }
   ],
   "source": [
    "# Test suite - reasoning problems\n",
    "TESTS = [\n",
    "    # Logic puzzles\n",
    "    {\"name\": \"River Crossing\", \"difficulty\": \"medium\",\n",
    "     \"prompt\": \"A farmer needs to cross a river with a wolf, goat, and cabbage. Boat carries farmer + 1 item. Wolf eats goat if alone. Goat eats cabbage if alone. How to cross safely?\",\n",
    "     \"expected\": \"goat\", \"answer_type\": \"contains\"},  # First trip must be goat\n",
    "    \n",
    "    {\"name\": \"Knights and Knaves\", \"difficulty\": \"medium\",\n",
    "     \"prompt\": \"On an island, knights always tell truth, knaves always lie. Person A says 'We are both knaves.' What are A and B?\",\n",
    "     \"expected\": \"knave\", \"answer_type\": \"contains\"},  # A must be a knave\n",
    "    \n",
    "    # Math reasoning\n",
    "    {\"name\": \"Simple Algebra\", \"difficulty\": \"easy\",\n",
    "     \"prompt\": \"Solve for x: 2x + 5 = 13. Show your reasoning.\",\n",
    "     \"expected\": \"4\", \"answer_type\": \"number\"},\n",
    "    \n",
    "    {\"name\": \"Quadratic\", \"difficulty\": \"medium\",\n",
    "     \"prompt\": \"Solve x¬≤ - 5x + 6 = 0. Show your reasoning.\",\n",
    "     \"expected\": \"2\", \"answer_type\": \"contains\"},  # x = 2 or x = 3\n",
    "    \n",
    "    {\"name\": \"Word Problem\", \"difficulty\": \"medium\",\n",
    "     \"prompt\": \"Train A leaves at 9am going 60mph. Train B leaves at 10am from 300mi away going 80mph toward A. When do they meet?\",\n",
    "     \"expected\": \"11:42\", \"answer_type\": \"contains\"},\n",
    "    \n",
    "    # Deductive reasoning\n",
    "    {\"name\": \"Syllogism\", \"difficulty\": \"easy\",\n",
    "     \"prompt\": \"All roses are flowers. Some flowers fade quickly. Can we conclude some roses fade quickly? Explain.\",\n",
    "     \"expected\": \"no\", \"answer_type\": \"contains\"},  # Invalid syllogism\n",
    "    \n",
    "    {\"name\": \"Set Logic\", \"difficulty\": \"medium\",\n",
    "     \"prompt\": \"Set A = {1,2,3,4,5}. Set B = {4,5,6,7}. What is A ‚à© B (intersection)?\",\n",
    "     \"expected\": \"{4,5}\", \"answer_type\": \"contains\"},\n",
    "    \n",
    "    # Programming logic\n",
    "    {\"name\": \"Big-O Complexity\", \"difficulty\": \"easy\",\n",
    "     \"prompt\": \"What is the time complexity of binary search? Explain why.\",\n",
    "     \"expected\": \"log\", \"answer_type\": \"contains\"},  # O(log n)\n",
    "    \n",
    "    {\"name\": \"Recursion Trace\", \"difficulty\": \"medium\",\n",
    "     \"prompt\": \"What does fib(5) return if fib(n) = fib(n-1) + fib(n-2), fib(0)=0, fib(1)=1?\",\n",
    "     \"expected\": \"5\", \"answer_type\": \"number\"},\n",
    "    \n",
    "    # Edge case reasoning\n",
    "    {\"name\": \"Edge Case Analysis\", \"difficulty\": \"easy\",\n",
    "     \"prompt\": \"What edge cases should you consider for a function that divides two numbers?\",\n",
    "     \"expected\": \"zero\", \"answer_type\": \"contains\"},  # Division by zero\n",
    "]\n",
    "print(f\"‚úì {len(TESTS)} reasoning tests defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:05:46.481953Z",
     "iopub.status.busy": "2025-12-30T22:05:46.481897Z",
     "iopub.status.idle": "2025-12-30T22:08:25.908579Z",
     "shell.execute_reply": "2025-12-30T22:08:25.907078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Reasoning Quality Analysis\n",
      "============================================================\n",
      "  Running: River Crossing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚ùå Score: 40 | Steps: 57 | 23.8s\n",
      "  Running: Knights and Knaves...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Score: 80 | Steps: 16 | 12.3s\n",
      "  Running: Simple Algebra...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚ùå Score: 10 | Steps: 6 | 7.6s\n",
      "  Running: Quadratic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Score: 90 | Steps: 7 | 14.2s\n",
      "  Running: Word Problem...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚ùå Score: 30 | Steps: 38 | 24.3s\n",
      "  Running: Syllogism...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Score: 90 | Steps: 22 | 21.1s\n",
      "  Running: Set Logic...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚ùå Score: 20 | Steps: 12 | 12.6s\n",
      "  Running: Big-O Complexity...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚úÖ Score: 100 | Steps: 11 | 16.4s\n",
      "  Running: Recursion Trace...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚ùå Score: 10 | Steps: 4 | 3.4s\n",
      "  Running: Edge Case Analysis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚ùå Score: 30 | Steps: 47 | 23.6s\n",
      "\n",
      "============================================================\n",
      "‚úÖ Completed 10 tests\n"
     ]
    }
   ],
   "source": [
    "# Test runner\n",
    "def run_test(test):\n",
    "    print(f\"  Running: {test['name']}...\")\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        response = client.chat([\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Think through problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": test['prompt']}\n",
    "        ], max_tokens=2048, temperature=0.3)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        \n",
    "        content = response.choices[0].message.content\n",
    "        thinking = extract_thinking(content)\n",
    "        answer = extract_answer(content)\n",
    "        reasoning = analyze_reasoning(thinking)\n",
    "        \n",
    "        correct = check_answer(answer, test['expected'], test['answer_type'])\n",
    "        \n",
    "        # Score: 50% correctness, 50% reasoning quality\n",
    "        reasoning_score = (\n",
    "            (20 if reasoning.step_count >= 3 else reasoning.step_count * 7) +\n",
    "            (20 if reasoning.has_self_correction else 0) +\n",
    "            (20 if reasoning.considers_alternatives else 0) +\n",
    "            (20 if reasoning.mentions_edge_cases else 0) +\n",
    "            (20 if reasoning.shows_verification else 0)\n",
    "        )\n",
    "        score = (50 if correct else 0) + reasoning_score * 0.5\n",
    "        \n",
    "        return TestResult(name=test['name'], passed=correct, score=min(score, 100),\n",
    "                         answer_correct=correct, reasoning=reasoning,\n",
    "                         thinking_content=thinking[:500], final_answer=answer[:200],\n",
    "                         expected_answer=test['expected'], completion_time=elapsed,\n",
    "                         tokens_used=response.usage.completion_tokens)\n",
    "    except Exception as e:\n",
    "        return TestResult(name=test['name'], passed=False, score=0, answer_correct=False,\n",
    "                         reasoning=ReasoningMetrics(), final_answer=f\"Error: {e}\", expected_answer=test['expected'])\n",
    "\n",
    "# Run tests\n",
    "print(\"üöÄ Running Reasoning Quality Analysis\")\n",
    "print(\"=\" * 60)\n",
    "results = BenchmarkResults(notebook=\"08_reasoning_quality\", timestamp=datetime.now().isoformat())\n",
    "\n",
    "for test in TESTS:\n",
    "    result = run_test(test)\n",
    "    results.tests.append(result)\n",
    "    status = \"‚úÖ\" if result.passed else \"‚ùå\"\n",
    "    steps = result.reasoning.step_count if result.reasoning else 0\n",
    "    print(f\"    {status} Score: {result.score:.0f} | Steps: {steps} | {result.completion_time:.1f}s\")\n",
    "\n",
    "print(f\"\\n{'='*60}\\n‚úÖ Completed {len(results.tests)} tests\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:25.919982Z",
     "iopub.status.busy": "2025-12-30T22:08:25.919751Z",
     "iopub.status.idle": "2025-12-30T22:08:25.931640Z",
     "shell.execute_reply": "2025-12-30T22:08:25.931029Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üìä Results Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Overall Statistics:\n",
      "   Answer Accuracy: 4/10 (40.0%)\n",
      "   Average Score: 50.0/100\n",
      "   Avg Reasoning Steps: 22.0\n",
      "   Self-Correction Rate: 50.0%\n",
      "\n",
      "üß† Reasoning Quality Metrics:\n",
      "   Self-Correction: 5/10 (50%)\n",
      "   Alternatives: 6/10 (60%)\n",
      "   Edge Cases: 4/10 (40%)\n",
      "   Verification: 5/10 (50%)\n",
      "\n",
      "üìä By Difficulty:\n",
      "   Easy: 50% accurate\n",
      "   Medium: 33% accurate\n",
      "\n",
      "Test                   Correct   Score   Steps  Self-Corr \n",
      "------------------------------------------------------------\n",
      "River Crossing            ‚ùå        40     57       ‚úì     \n",
      "Knights and Knaves        ‚úÖ        80     16             \n",
      "Simple Algebra            ‚ùå        10     6              \n",
      "Quadratic                 ‚úÖ        90     7              \n",
      "Word Problem              ‚ùå        30     38             \n",
      "Syllogism                 ‚úÖ        90     22       ‚úì     \n",
      "Set Logic                 ‚ùå        20     12       ‚úì     \n",
      "Big-O Complexity          ‚úÖ       100     11       ‚úì     \n",
      "Recursion Trace           ‚ùå        10     4              \n",
      "Edge Case Analysis        ‚ùå        30     47       ‚úì     \n"
     ]
    }
   ],
   "source": [
    "# Results summary\n",
    "display(Markdown(\"## üìä Results Summary\"))\n",
    "\n",
    "correct = sum(1 for t in results.tests if t.answer_correct)\n",
    "total = len(results.tests)\n",
    "\n",
    "print(f\"\\nüìà Overall Statistics:\")\n",
    "print(f\"   Answer Accuracy: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "print(f\"   Average Score: {results.avg_score:.1f}/100\")\n",
    "print(f\"   Avg Reasoning Steps: {results.avg_reasoning_steps:.1f}\")\n",
    "print(f\"   Self-Correction Rate: {results.self_correction_rate:.1f}%\")\n",
    "\n",
    "# Reasoning quality breakdown\n",
    "print(f\"\\nüß† Reasoning Quality Metrics:\")\n",
    "metrics_summary = {\n",
    "    'Self-Correction': sum(1 for t in results.tests if t.reasoning and t.reasoning.has_self_correction),\n",
    "    'Alternatives': sum(1 for t in results.tests if t.reasoning and t.reasoning.considers_alternatives),\n",
    "    'Edge Cases': sum(1 for t in results.tests if t.reasoning and t.reasoning.mentions_edge_cases),\n",
    "    'Verification': sum(1 for t in results.tests if t.reasoning and t.reasoning.shows_verification),\n",
    "}\n",
    "for name, count in metrics_summary.items():\n",
    "    print(f\"   {name}: {count}/{total} ({count/total*100:.0f}%)\")\n",
    "\n",
    "# By difficulty\n",
    "print(f\"\\nüìä By Difficulty:\")\n",
    "for diff in ['easy', 'medium']:\n",
    "    diff_tests = [t for t in results.tests if any(test['difficulty'] == diff and test['name'] == t.name for test in TESTS)]\n",
    "    if diff_tests:\n",
    "        acc = sum(1 for t in diff_tests if t.answer_correct) / len(diff_tests) * 100\n",
    "        print(f\"   {diff.title()}: {acc:.0f}% accurate\")\n",
    "\n",
    "# Detailed table\n",
    "print(f\"\\n{'Test':<22} {'Correct':^8} {'Score':^8} {'Steps':^6} {'Self-Corr':^10}\")\n",
    "print(\"-\" * 60)\n",
    "for t in results.tests:\n",
    "    r = t.reasoning if t.reasoning else ReasoningMetrics()\n",
    "    print(f\"{t.name:<22} {'‚úÖ' if t.answer_correct else '‚ùå':^8} {t.score:>5.0f}   {r.step_count:^6} {'‚úì' if r.has_self_correction else '':^10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:08:25.933096Z",
     "iopub.status.busy": "2025-12-30T22:08:25.932990Z",
     "iopub.status.idle": "2025-12-30T22:08:25.939113Z",
     "shell.execute_reply": "2025-12-30T22:08:25.938658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to benchmark_results/08_reasoning_quality.json\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## üìã Feedback Summary\n",
       "\n",
       "**Model**: MiniMax-M2.1 | **Date**: 2025-12-30\n",
       "\n",
       "| Metric | Value |\n",
       "|--------|-------|\n",
       "| Answer Accuracy | 4/10 (40%) |\n",
       "| Avg Reasoning Steps | 22.0 |\n",
       "| Self-Correction Rate | 50% |\n",
       "| Average Score | 50/100 |\n",
       "\n",
       "### Reasoning Quality Breakdown:\n",
       "- Self-Correction: 5/10 (50%)\n",
       "- Considers Alternatives: 6/10 (60%)\n",
       "- Edge Case Awareness: 4/10 (40%)\n",
       "- Verification: 5/10 (50%)\n",
       "\n",
       "**Key Insight**: MiniMax's `<think>` blocks provide deep reasoning transparency - a unique differentiator.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save results\n",
    "os.makedirs(\"benchmark_results\", exist_ok=True)\n",
    "\n",
    "output = {\n",
    "    'notebook': results.notebook, 'timestamp': results.timestamp,\n",
    "    'summary': {\n",
    "        'accuracy': correct/total*100, 'avg_score': results.avg_score,\n",
    "        'avg_reasoning_steps': results.avg_reasoning_steps,\n",
    "        'self_correction_rate': results.self_correction_rate,\n",
    "        'reasoning_metrics': metrics_summary\n",
    "    },\n",
    "    'tests': [{'name': t.name, 'correct': t.answer_correct, 'score': t.score,\n",
    "               'reasoning': asdict(t.reasoning) if t.reasoning else None,\n",
    "               'thinking_preview': t.thinking_content[:200] if t.thinking_content else \"\",\n",
    "               'answer': t.final_answer[:100], 'expected': t.expected_answer,\n",
    "               'time': t.completion_time, 'tokens': t.tokens_used} for t in results.tests]\n",
    "}\n",
    "\n",
    "with open(\"benchmark_results/08_reasoning_quality.json\", 'w') as f:\n",
    "    json.dump(output, f, indent=2, default=str)\n",
    "print(\"‚úÖ Results saved to benchmark_results/08_reasoning_quality.json\")\n",
    "\n",
    "# Summary for feedback\n",
    "display(Markdown(f\"\"\"\n",
    "## üìã Feedback Summary\n",
    "\n",
    "**Model**: {client.model} | **Date**: {results.timestamp[:10]}\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Answer Accuracy | {correct}/{total} ({correct/total*100:.0f}%) |\n",
    "| Avg Reasoning Steps | {results.avg_reasoning_steps:.1f} |\n",
    "| Self-Correction Rate | {results.self_correction_rate:.0f}% |\n",
    "| Average Score | {results.avg_score:.0f}/100 |\n",
    "\n",
    "### Reasoning Quality Breakdown:\n",
    "- Self-Correction: {metrics_summary['Self-Correction']}/{total} ({metrics_summary['Self-Correction']/total*100:.0f}%)\n",
    "- Considers Alternatives: {metrics_summary['Alternatives']}/{total} ({metrics_summary['Alternatives']/total*100:.0f}%)\n",
    "- Edge Case Awareness: {metrics_summary['Edge Cases']}/{total} ({metrics_summary['Edge Cases']/total*100:.0f}%)\n",
    "- Verification: {metrics_summary['Verification']}/{total} ({metrics_summary['Verification']/total*100:.0f}%)\n",
    "\n",
    "**Key Insight**: MiniMax's `<think>` blocks provide {'deep' if results.avg_reasoning_steps > 5 else 'moderate'} reasoning transparency - a unique differentiator.\n",
    "\"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
