{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import HTML, display, Markdown\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Import clients\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "from src.minimax_client import MiniMaxClient\n",
    "\n",
    "# Check for pandas\n",
    "try:\n",
    "    import pandas as pd\n",
    "    HAS_PANDAS = True\n",
    "except ImportError:\n",
    "    HAS_PANDAS = False\n",
    "    print(\"âš ï¸ pandas not installed - tables will use text format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CompletionResult:\n",
    "    \"\"\"Stores results from a model completion.\"\"\"\n",
    "    model_name: str\n",
    "    provider: str\n",
    "    content: str\n",
    "    completion_time: float\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def tokens_per_second(self) -> float:\n",
    "        return self.completion_tokens / self.completion_time if self.completion_time > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def success(self) -> bool:\n",
    "        return self.error is None\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        if self.error:\n",
    "            return f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  {self.provider}: {self.model_name}\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  âŒ Error: {self.error[:50]}...\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "        return f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  {self.provider}: {self.model_name}\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  â±ï¸  Completion Time:     {self.completion_time:.2f}s\n",
    "â•‘  ğŸ“ Prompt Tokens:        {self.prompt_tokens:,}\n",
    "â•‘  âœï¸  Completion Tokens:   {self.completion_tokens:,}\n",
    "â•‘  ğŸ“Š Total Tokens:         {self.total_tokens:,}\n",
    "â•‘  âš¡ Speed:                {self.tokens_per_second:.1f} tokens/sec\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Œ Initialized clients:\n",
      "   âœ“ MiniMax: Ready\n",
      "   âœ“ OpenAI: Ready\n",
      "   âœ“ Anthropic: Ready\n"
     ]
    }
   ],
   "source": [
    "class MultiModelClient:\n",
    "    \"\"\"Client for comparing completions across multiple LLM providers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # MiniMax\n",
    "        try:\n",
    "            self.minimax = MiniMaxClient()\n",
    "            self.minimax_ready = True\n",
    "        except Exception as e:\n",
    "            self.minimax = None\n",
    "            self.minimax_ready = False\n",
    "            print(f\"âš ï¸ MiniMax init failed: {e}\")\n",
    "        \n",
    "        # OpenAI\n",
    "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.openai = OpenAI(api_key=openai_key) if openai_key else None\n",
    "        self.openai_ready = openai_key is not None\n",
    "        \n",
    "        # Anthropic\n",
    "        anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        self.anthropic = anthropic.Anthropic(api_key=anthropic_key) if anthropic_key else None\n",
    "        self.anthropic_ready = anthropic_key is not None\n",
    "        \n",
    "        print(\"ğŸ”Œ Initialized clients:\")\n",
    "        print(f\"   {'âœ“' if self.minimax_ready else 'âœ—'} MiniMax: {'Ready' if self.minimax_ready else 'No API key'}\")\n",
    "        print(f\"   {'âœ“' if self.openai_ready else 'âœ—'} OpenAI: {'Ready' if self.openai_ready else 'No API key'}\")\n",
    "        print(f\"   {'âœ“' if self.anthropic_ready else 'âœ—'} Anthropic: {'Ready' if self.anthropic_ready else 'No API key'}\")\n",
    "    \n",
    "    def complete_minimax(self, prompt: str, system: str, model: str = \"MiniMax-M2.1\", max_tokens: int = 16000) -> CompletionResult:\n",
    "        if not self.minimax_ready:\n",
    "            return CompletionResult(model_name=model, provider=\"MiniMax\", content=\"\", completion_time=0, \n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=\"Client not initialized\")\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            response = self.minimax.chat(messages, model=model, max_tokens=max_tokens, temperature=0.7)\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            return CompletionResult(\n",
    "                model_name=model, provider=\"MiniMax\",\n",
    "                content=response.choices[0].message.content,\n",
    "                completion_time=elapsed,\n",
    "                prompt_tokens=response.usage.prompt_tokens,\n",
    "                completion_tokens=response.usage.completion_tokens,\n",
    "                total_tokens=response.usage.total_tokens\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return CompletionResult(model_name=model, provider=\"MiniMax\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=str(e))\n",
    "    \n",
    "    def complete_openai(self, prompt: str, system: str, model: str = \"gpt-4o\", max_tokens: int = 16000) -> CompletionResult:\n",
    "        if not self.openai_ready:\n",
    "            return CompletionResult(model_name=model, provider=\"OpenAI\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=\"Set OPENAI_API_KEY\")\n",
    "        \n",
    "        is_o1 = model.startswith(\"o1\")\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"{system}\\n\\n{prompt}\"}] if is_o1 else [\n",
    "            {\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            kwargs = {\"model\": model, \"messages\": messages, \"max_completion_tokens\": max_tokens}\n",
    "            if not is_o1:\n",
    "                kwargs[\"temperature\"] = 0.7\n",
    "            response = self.openai.chat.completions.create(**kwargs)\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            return CompletionResult(\n",
    "                model_name=model, provider=\"OpenAI\",\n",
    "                content=response.choices[0].message.content,\n",
    "                completion_time=elapsed,\n",
    "                prompt_tokens=response.usage.prompt_tokens,\n",
    "                completion_tokens=response.usage.completion_tokens,\n",
    "                total_tokens=response.usage.total_tokens\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return CompletionResult(model_name=model, provider=\"OpenAI\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=str(e))\n",
    "    \n",
    "    def complete_anthropic(self, prompt: str, system: str, model: str = \"claude-sonnet-4-20250514\", max_tokens: int = 16000) -> CompletionResult:\n",
    "        if not self.anthropic_ready:\n",
    "            return CompletionResult(model_name=model, provider=\"Anthropic\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=\"Set ANTHROPIC_API_KEY\")\n",
    "        \n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            response = self.anthropic.messages.create(\n",
    "                model=model, max_tokens=max_tokens, system=system,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            return CompletionResult(\n",
    "                model_name=model, provider=\"Anthropic\",\n",
    "                content=response.content[0].text,\n",
    "                completion_time=elapsed,\n",
    "                prompt_tokens=response.usage.input_tokens,\n",
    "                completion_tokens=response.usage.output_tokens,\n",
    "                total_tokens=response.usage.input_tokens + response.usage.output_tokens\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return CompletionResult(model_name=model, provider=\"Anthropic\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=str(e))\n",
    "\n",
    "# Initialize client\n",
    "client = MultiModelClient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Select Models to Compare\n",
    "\n",
    "Choose which models to include in the comparison:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Œ Models selected for comparison:\n",
      "\n",
      "  MINIMAX:\n",
      "    â€¢ MiniMax-M2.1\n",
      "\n",
      "  OPENAI:\n",
      "    â€¢ gpt-4o\n",
      "\n",
      "  ANTHROPIC:\n",
      "    â€¢ claude-sonnet-4-20250514\n",
      "\n",
      "  Total: 3 models\n"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ›ï¸ CONFIGURATION: Select which models to compare\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "MODELS_TO_COMPARE = {\n",
    "    # MiniMax\n",
    "    \"minimax\": [\n",
    "        \"MiniMax-M2.1\",\n",
    "    ],\n",
    "    \n",
    "    # OpenAI - uncomment models you want to test\n",
    "    \"openai\": [\n",
    "        \"gpt-4o\",\n",
    "        # \"gpt-4o-mini\",        # Faster, cheaper\n",
    "        # \"gpt-4-turbo\",        # Previous flagship\n",
    "        # \"o1\",                 # Reasoning model (very slow for code gen)\n",
    "    ],\n",
    "    \n",
    "    # Anthropic - uncomment models you want to test\n",
    "    \"anthropic\": [\n",
    "        \"claude-sonnet-4-20250514\",   # Latest Sonnet\n",
    "        # \"claude-3-5-sonnet-20241022\", # Previous Sonnet\n",
    "        # \"claude-3-opus-20240229\",     # Most capable (expensive)\n",
    "        # \"claude-3-5-haiku-20241022\",  # Fast and cheap\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Œ Models selected for comparison:\")\n",
    "total = 0\n",
    "for provider, models in MODELS_TO_COMPARE.items():\n",
    "    if models:\n",
    "        print(f\"\\n  {provider.upper()}:\")\n",
    "        for model in models:\n",
    "            print(f\"    â€¢ {model}\")\n",
    "            total += 1\n",
    "print(f\"\\n  Total: {total} models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Next.js Application Prompt\n",
    "\n",
    "A complex Next.js 14 application with:\n",
    "- TypeScript throughout\n",
    "- App Router architecture\n",
    "- Multiple reusable components\n",
    "- State management with Context\n",
    "- Tailwind CSS styling\n",
    "- Kanban board with drag & drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Prompt ready!\n",
      "   System prompt: 781 chars\n",
      "   User prompt: 2,385 chars\n",
      "   Total: 3,166 chars\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a senior full-stack developer specializing in Next.js 14+, React 18+, and TypeScript.\n",
    "\n",
    "Generate production-ready code following these standards:\n",
    "- Use Next.js App Router (app/ directory structure)\n",
    "- TypeScript with strict types - no 'any' types\n",
    "- React Server Components where appropriate\n",
    "- Client Components with 'use client' directive when needed\n",
    "- Tailwind CSS for all styling\n",
    "- Proper error handling and loading states\n",
    "- Accessible HTML (ARIA labels, semantic elements)\n",
    "- Performance optimized (lazy loading, memoization where needed)\n",
    "\n",
    "Output format: Provide complete file contents with clear file path comments.\n",
    "Use this format for each file:\n",
    "// === FILE: path/to/file.tsx ===\n",
    "<file contents>\n",
    "// === END FILE ===\n",
    "\n",
    "Generate ALL required files for a working application.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Create a complete Next.js 14 application: \"TaskFlow\" - A modern task management dashboard.\n",
    "\n",
    "## Required Features:\n",
    "\n",
    "### 1. Layout & Navigation (app/layout.tsx)\n",
    "- Dark theme with gradient accents (slate-900 to slate-800)\n",
    "- Responsive sidebar navigation with icons\n",
    "- Collapsible sidebar on mobile\n",
    "- Header with search bar and user avatar dropdown\n",
    "- Breadcrumb navigation\n",
    "\n",
    "### 2. Dashboard Page (app/page.tsx)\n",
    "- Overview cards: Total Tasks, Completed, In Progress, Overdue (with animated counters)\n",
    "- Task completion chart (simple CSS-based bar chart)\n",
    "- Recent activity feed with timestamps\n",
    "- Quick-add task floating button\n",
    "\n",
    "### 3. Task Board Page (app/tasks/page.tsx)\n",
    "- Kanban-style board with 4 columns: Backlog, To Do, In Progress, Done\n",
    "- Drag and drop tasks between columns (implement with React state, no external libs)\n",
    "- Task cards showing: title, priority badge, due date, assignee avatar\n",
    "- Filter by priority and search\n",
    "- Add new task modal\n",
    "\n",
    "### 4. Components Required:\n",
    "- TaskCard.tsx - Individual task with hover effects, priority colors\n",
    "- KanbanColumn.tsx - Column with task count and drop zone styling\n",
    "- Modal.tsx - Reusable modal with animations\n",
    "- Button.tsx - Styled button with variants (primary, secondary, danger)\n",
    "- Badge.tsx - Priority/status badges\n",
    "- Avatar.tsx - User avatar with fallback initials\n",
    "- SearchInput.tsx - Animated search with icon\n",
    "- Sidebar.tsx - Navigation with active states\n",
    "\n",
    "### 5. State Management:\n",
    "- Use React Context for task state (TaskContext.tsx)\n",
    "- Custom hooks: useTask, useDragDrop\n",
    "- Persist to localStorage\n",
    "\n",
    "### 6. Types (types/index.ts):\n",
    "```typescript\n",
    "interface Task {\n",
    "  id: string;\n",
    "  title: string;\n",
    "  description?: string;\n",
    "  status: 'backlog' | 'todo' | 'in-progress' | 'done';\n",
    "  priority: 'low' | 'medium' | 'high' | 'urgent';\n",
    "  dueDate?: string;\n",
    "  assignee?: User;\n",
    "  createdAt: string;\n",
    "}\n",
    "\n",
    "interface User {\n",
    "  id: string;\n",
    "  name: string;\n",
    "  avatar?: string;\n",
    "}\n",
    "```\n",
    "\n",
    "### 7. Styling Requirements:\n",
    "- Glassmorphism cards with backdrop-blur\n",
    "- Smooth transitions on all interactive elements (300ms)\n",
    "- Gradient borders on focused inputs\n",
    "- Hover lift effect on cards (transform + shadow)\n",
    "- Loading skeletons for async content\n",
    "- Toast notifications for actions\n",
    "\n",
    "### 8. Sample Data:\n",
    "Include realistic sample tasks (8-10 tasks across all columns) with varied priorities and due dates.\n",
    "\n",
    "Generate a complete, working Next.js application with all files needed.\"\"\"\n",
    "\n",
    "print(\"ğŸ“ Prompt ready!\")\n",
    "print(f\"   System prompt: {len(SYSTEM_PROMPT):,} chars\")\n",
    "print(f\"   User prompt: {len(USER_PROMPT):,} chars\")\n",
    "print(f\"   Total: {len(SYSTEM_PROMPT) + len(USER_PROMPT):,} chars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Next.js application generation comparison...\n",
      "======================================================================\n",
      "âš ï¸  This is a complex prompt - expect 30-120 seconds per model!\n",
      "\n",
      "\n",
      "â³ Running MiniMax: MiniMax-M2.1...\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘  MiniMax: MiniMax-M2.1\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘  â±ï¸  Completion Time:     126.03s\n",
      "â•‘  ğŸ“ Prompt Tokens:        744\n",
      "â•‘  âœï¸  Completion Tokens:   12,547\n",
      "â•‘  ğŸ“Š Total Tokens:         13,291\n",
      "â•‘  âš¡ Speed:                99.6 tokens/sec\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "\n",
      "â³ Running OpenAI: gpt-4o...\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘  OpenAI: gpt-4o\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘  â±ï¸  Completion Time:     16.17s\n",
      "â•‘  ğŸ“ Prompt Tokens:        742\n",
      "â•‘  âœï¸  Completion Tokens:   1,964\n",
      "â•‘  ğŸ“Š Total Tokens:         2,706\n",
      "â•‘  âš¡ Speed:                121.5 tokens/sec\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "\n",
      "â³ Running Anthropic: claude-sonnet-4-20250514...\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘  Anthropic: claude-sonnet-4-20250514\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
      "â•‘  âŒ Error: Error code: 400 - {'type': 'error', 'error': {'typ...\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "\n",
      "======================================================================\n",
      "âœ… Completed 3 model comparisons!\n"
     ]
    }
   ],
   "source": [
    "# Run the comparison\n",
    "results: dict[str, CompletionResult] = {}\n",
    "\n",
    "print(\"ğŸš€ Starting Next.js application generation comparison...\")\n",
    "print(\"=\" * 70)\n",
    "print(\"âš ï¸  This is a complex prompt - expect 30-120 seconds per model!\\n\")\n",
    "\n",
    "# MiniMax\n",
    "for model in MODELS_TO_COMPARE.get(\"minimax\", []):\n",
    "    print(f\"\\nâ³ Running MiniMax: {model}...\")\n",
    "    result = client.complete_minimax(USER_PROMPT, SYSTEM_PROMPT, model=model)\n",
    "    results[f\"minimax_{model}\"] = result\n",
    "    print(result.summary())\n",
    "\n",
    "# OpenAI\n",
    "for model in MODELS_TO_COMPARE.get(\"openai\", []):\n",
    "    print(f\"\\nâ³ Running OpenAI: {model}...\")\n",
    "    result = client.complete_openai(USER_PROMPT, SYSTEM_PROMPT, model=model)\n",
    "    results[f\"openai_{model}\"] = result\n",
    "    print(result.summary())\n",
    "\n",
    "# Anthropic\n",
    "for model in MODELS_TO_COMPARE.get(\"anthropic\", []):\n",
    "    print(f\"\\nâ³ Running Anthropic: {model}...\")\n",
    "    result = client.complete_anthropic(USER_PROMPT, SYSTEM_PROMPT, model=model)\n",
    "    results[f\"anthropic_{model}\"] = result\n",
    "    print(result.summary())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"âœ… Completed {len(results)} model comparisons!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ğŸ“Š Comparison Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provider</th>\n",
       "      <th>Model</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Speed (tok/s)</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MiniMax</td>\n",
       "      <td>MiniMax-M2.1</td>\n",
       "      <td>126.03</td>\n",
       "      <td>12,547</td>\n",
       "      <td>99.6</td>\n",
       "      <td>47,535 chars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenAI</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>16.17</td>\n",
       "      <td>1,964</td>\n",
       "      <td>121.5</td>\n",
       "      <td>7,610 chars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Provider         Model Time (s)  Tokens Speed (tok/s)        Output\n",
       "0  MiniMax  MiniMax-M2.1   126.03  12,547          99.6  47,535 chars\n",
       "1   OpenAI        gpt-4o    16.17   1,964         121.5   7,610 chars"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ† Results:\n",
      "   âš¡ Fastest: OpenAI gpt-4o (16.17s)\n",
      "   ğŸš€ Highest speed: OpenAI gpt-4o (121.5 tok/s)\n",
      "   ğŸ“ Most code: MiniMax MiniMax-M2.1 (12,547 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Comparison summary\n",
    "successful = {k: v for k, v in results.items() if v.success}\n",
    "\n",
    "if successful:\n",
    "    display(Markdown(\"## ğŸ“Š Comparison Summary\"))\n",
    "    \n",
    "    if HAS_PANDAS:\n",
    "        data = [{\n",
    "            'Provider': r.provider,\n",
    "            'Model': r.model_name,\n",
    "            'Time (s)': f\"{r.completion_time:.2f}\",\n",
    "            'Tokens': f\"{r.completion_tokens:,}\",\n",
    "            'Speed (tok/s)': f\"{r.tokens_per_second:.1f}\",\n",
    "            'Output': f\"{len(r.content):,} chars\"\n",
    "        } for r in successful.values()]\n",
    "        display(pd.DataFrame(data))\n",
    "    else:\n",
    "        print(f\"{'Provider':<12} {'Model':<28} {'Time':<10} {'Tokens':<10} {'Speed':<12}\")\n",
    "        print(\"-\" * 75)\n",
    "        for r in successful.values():\n",
    "            print(f\"{r.provider:<12} {r.model_name:<28} {r.completion_time:.2f}s     {r.completion_tokens:<10,} {r.tokens_per_second:.1f} tok/s\")\n",
    "    \n",
    "    # Winners\n",
    "    fastest = min(successful.values(), key=lambda x: x.completion_time)\n",
    "    most_output = max(successful.values(), key=lambda x: x.completion_tokens)\n",
    "    highest_speed = max(successful.values(), key=lambda x: x.tokens_per_second)\n",
    "    \n",
    "    print(f\"\\nğŸ† Results:\")\n",
    "    print(f\"   âš¡ Fastest: {fastest.provider} {fastest.model_name} ({fastest.completion_time:.2f}s)\")\n",
    "    print(f\"   ğŸš€ Highest speed: {highest_speed.provider} {highest_speed.model_name} ({highest_speed.tokens_per_second:.1f} tok/s)\")\n",
    "    print(f\"   ğŸ“ Most code: {most_output.provider} {most_output.model_name} ({most_output.completion_tokens:,} tokens)\")\n",
    "else:\n",
    "    print(\"âŒ No successful completions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ğŸ” Code Quality Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MiniMax - MiniMax-M2.1\n",
      "-------------------------------------------------------\n",
      "  Lines               : 1286\n",
      "  Characters          : 42120\n",
      "  Files Found         : 19\n",
      "  TypeScript          : âœ“\n",
      "  App Router          : âœ“\n",
      "  Client Components   : âœ“\n",
      "  React Hooks         : âœ“\n",
      "  Custom Hooks        : âœ“\n",
      "  Tailwind            : âœ“\n",
      "  Context API         : âœ“\n",
      "  Type Definitions    : âœ“\n",
      "  Animations          : âœ“\n",
      "  Accessibility       : âœ—\n",
      "\n",
      "OpenAI - gpt-4o\n",
      "-------------------------------------------------------\n",
      "  Lines               : 277\n",
      "  Characters          : 7610\n",
      "  Files Found         : 14\n",
      "  TypeScript          : âœ“\n",
      "  App Router          : âœ“\n",
      "  Client Components   : âœ“\n",
      "  React Hooks         : âœ“\n",
      "  Custom Hooks        : âœ“\n",
      "  Tailwind            : âœ“\n",
      "  Context API         : âœ“\n",
      "  Type Definitions    : âœ“\n",
      "  Animations          : âœ“\n",
      "  Accessibility       : âœ—\n"
     ]
    }
   ],
   "source": [
    "# Code quality analysis\n",
    "def analyze_code(content: str) -> dict:\n",
    "    \"\"\"Analyze Next.js code output for completeness.\"\"\"\n",
    "    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)\n",
    "    lower = content.lower()\n",
    "    \n",
    "    files = re.findall(r'(?:// ===|/\\*\\*|###|File:)\\s*(?:FILE:?)?\\s*([\\w./\\-]+\\.(?:tsx?|js|css|json))', content, re.I)\n",
    "    \n",
    "    return {\n",
    "        'Lines': len(content.split('\\n')),\n",
    "        'Characters': len(content),\n",
    "        'Files Found': len(set(files)) if files else '?',\n",
    "        'TypeScript': 'âœ“' if ('.tsx' in content or 'interface ' in content) else 'âœ—',\n",
    "        'App Router': 'âœ“' if ('app/' in content) else 'âœ—',\n",
    "        'Client Components': 'âœ“' if (\"'use client'\" in content) else 'âœ—',\n",
    "        'React Hooks': 'âœ“' if any(h in lower for h in ['usestate', 'useeffect', 'usecontext']) else 'âœ—',\n",
    "        'Custom Hooks': 'âœ“' if ('function use' in lower or 'const use' in lower) else 'âœ—',\n",
    "        'Tailwind': 'âœ“' if any(c in content for c in ['className=', 'bg-', 'flex ', 'grid ']) else 'âœ—',\n",
    "        'Context API': 'âœ“' if 'createcontext' in lower else 'âœ—',\n",
    "        'Type Definitions': 'âœ“' if ('interface ' in content or 'type ' in content) else 'âœ—',\n",
    "        'Animations': 'âœ“' if ('transition' in lower or 'animate' in lower) else 'âœ—',\n",
    "        'Accessibility': 'âœ“' if ('aria-' in lower or 'role=' in lower) else 'âœ—',\n",
    "    }\n",
    "\n",
    "display(Markdown(\"## ğŸ” Code Quality Analysis\"))\n",
    "\n",
    "for name, result in results.items():\n",
    "    if result.success:\n",
    "        print(f\"\\n{result.provider} - {result.model_name}\")\n",
    "        print(\"-\" * 55)\n",
    "        analysis = analyze_code(result.content)\n",
    "        for metric, value in analysis.items():\n",
    "            print(f\"  {metric:<20}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ğŸ“ˆ Visual Comparison"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â±ï¸  Completion Time (lower = better)\n",
      "===========================================================================\n",
      "MiniMax MiniMax-M2.1 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 126.03s\n",
      "OpenAI gpt-4o        â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 16.17s\n",
      "\n",
      "\n",
      "âš¡ Generation Speed (higher = better)\n",
      "===========================================================================\n",
      "OpenAI gpt-4o        â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 121.49 tok/s\n",
      "MiniMax MiniMax-M2.1 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 99.56 tok/s\n",
      "\n",
      "\n",
      "ğŸ“ Output Size\n",
      "===========================================================================\n",
      "MiniMax MiniMax-M2.1 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 47.53 KB\n",
      "OpenAI gpt-4o        â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 7.61 KB\n",
      "\n",
      "\n",
      "ğŸ”¢ Tokens Generated\n",
      "===========================================================================\n",
      "MiniMax MiniMax-M2.1 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 12547.00\n",
      "OpenAI gpt-4o        â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1964.00\n"
     ]
    }
   ],
   "source": [
    "# Visual comparison charts\n",
    "def bar_chart(data: dict, title: str, unit: str = \"\", width: int = 40):\n",
    "    \"\"\"Simple ASCII bar chart.\"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    max_val = max(data.values())\n",
    "    max_label = max(len(k) for k in data.keys())\n",
    "    \n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * (max_label + width + 15))\n",
    "    for label, val in sorted(data.items(), key=lambda x: -x[1]):\n",
    "        bar = \"â–ˆ\" * int((val / max_val) * width) if max_val > 0 else \"\"\n",
    "        print(f\"{label:<{max_label}} â”‚ {bar} {val:.2f}{unit}\")\n",
    "\n",
    "if successful:\n",
    "    display(Markdown(\"## ğŸ“ˆ Visual Comparison\"))\n",
    "    \n",
    "    bar_chart({f\"{r.provider} {r.model_name[:18]}\": r.completion_time for r in successful.values()},\n",
    "              \"â±ï¸  Completion Time (lower = better)\", \"s\")\n",
    "    \n",
    "    bar_chart({f\"{r.provider} {r.model_name[:18]}\": r.tokens_per_second for r in successful.values()},\n",
    "              \"\\nâš¡ Generation Speed (higher = better)\", \" tok/s\")\n",
    "    \n",
    "    bar_chart({f\"{r.provider} {r.model_name[:18]}\": len(r.content)/1000 for r in successful.values()},\n",
    "              \"\\nğŸ“ Output Size\", \" KB\")\n",
    "    \n",
    "    bar_chart({f\"{r.provider} {r.model_name[:18]}\": r.completion_tokens for r in successful.values()},\n",
    "              \"\\nğŸ”¢ Tokens Generated\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## ğŸ’¾ Save Generated Code"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… generated_nextjs/minimax_MiniMax_M2_1.md\n",
      "âœ… generated_nextjs/openai_gpt_4o.md\n",
      "\n",
      "ğŸ“ Saved 2 files to 'generated_nextjs/'\n",
      "   Open these files to compare the generated code!\n"
     ]
    }
   ],
   "source": [
    "# Save generated code to files\n",
    "output_dir = \"generated_nextjs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "display(Markdown(\"## ğŸ’¾ Save Generated Code\"))\n",
    "\n",
    "saved = []\n",
    "for name, result in results.items():\n",
    "    if result.success:\n",
    "        safe = result.model_name.replace('.', '_').replace('-', '_').replace(' ', '_')\n",
    "        filename = f\"{output_dir}/{result.provider.lower()}_{safe}.md\"\n",
    "        \n",
    "        content = re.sub(r'<think>.*?</think>', '', result.content, flags=re.DOTALL).strip()\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(f\"# TaskFlow - Generated by {result.provider} {result.model_name}\\n\\n\")\n",
    "            f.write(f\"**Time:** {result.completion_time:.2f}s | **Tokens:** {result.completion_tokens:,}\\n\\n---\\n\\n\")\n",
    "            f.write(content)\n",
    "        \n",
    "        saved.append(filename)\n",
    "        print(f\"âœ… {filename}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Saved {len(saved)} files to '{output_dir}/'\")\n",
    "print(\"   Open these files to compare the generated code!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
