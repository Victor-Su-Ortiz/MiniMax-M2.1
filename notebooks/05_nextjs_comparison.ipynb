{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T21:58:47.777708Z",
     "iopub.status.busy": "2025-12-30T21:58:47.777374Z",
     "iopub.status.idle": "2025-12-30T21:58:48.242233Z",
     "shell.execute_reply": "2025-12-30T21:58:48.241789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import HTML, display, Markdown\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Import clients\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "\n",
    "from src.minimax_client import MiniMaxClient\n",
    "\n",
    "# Check for pandas\n",
    "try:\n",
    "    import pandas as pd\n",
    "    HAS_PANDAS = True\n",
    "except ImportError:\n",
    "    HAS_PANDAS = False\n",
    "    print(\"\u26a0\ufe0f pandas not installed - tables will use text format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T21:58:48.243570Z",
     "iopub.status.busy": "2025-12-30T21:58:48.243477Z",
     "iopub.status.idle": "2025-12-30T21:58:48.246442Z",
     "shell.execute_reply": "2025-12-30T21:58:48.246046Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CompletionResult:\n",
    "    \"\"\"Stores results from a model completion.\"\"\"\n",
    "    model_name: str\n",
    "    provider: str\n",
    "    content: str\n",
    "    completion_time: float\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    error: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def tokens_per_second(self) -> float:\n",
    "        return self.completion_tokens / self.completion_time if self.completion_time > 0 else 0\n",
    "    \n",
    "    @property\n",
    "    def success(self) -> bool:\n",
    "        return self.error is None\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        if self.error:\n",
    "            return f\"\"\"\n",
    "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "\u2551  {self.provider}: {self.model_name}\n",
    "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n",
    "\u2551  \u274c Error: {self.error[:50]}...\n",
    "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\"\"\"\n",
    "        return f\"\"\"\n",
    "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "\u2551  {self.provider}: {self.model_name}\n",
    "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n",
    "\u2551  \u23f1\ufe0f  Completion Time:     {self.completion_time:.2f}s\n",
    "\u2551  \ud83d\udcdd Prompt Tokens:        {self.prompt_tokens:,}\n",
    "\u2551  \u270d\ufe0f  Completion Tokens:   {self.completion_tokens:,}\n",
    "\u2551  \ud83d\udcca Total Tokens:         {self.total_tokens:,}\n",
    "\u2551  \u26a1 Speed:                {self.tokens_per_second:.1f} tokens/sec\n",
    "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T21:58:48.247588Z",
     "iopub.status.busy": "2025-12-30T21:58:48.247532Z",
     "iopub.status.idle": "2025-12-30T21:58:48.289452Z",
     "shell.execute_reply": "2025-12-30T21:58:48.289016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0c Initialized clients:\n",
      "   \u2713 MiniMax: Ready\n",
      "   \u2713 OpenAI: Ready\n",
      "   \u2713 Anthropic: Ready\n"
     ]
    }
   ],
   "source": [
    "class MultiModelClient:\n",
    "    \"\"\"Client for comparing completions across multiple LLM providers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # MiniMax\n",
    "        try:\n",
    "            self.minimax = MiniMaxClient()\n",
    "            self.minimax_ready = True\n",
    "        except Exception as e:\n",
    "            self.minimax = None\n",
    "            self.minimax_ready = False\n",
    "            print(f\"\u26a0\ufe0f MiniMax init failed: {e}\")\n",
    "        \n",
    "        # OpenAI\n",
    "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.openai = OpenAI(api_key=openai_key) if openai_key else None\n",
    "        self.openai_ready = openai_key is not None\n",
    "        \n",
    "        # Anthropic\n",
    "        anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        self.anthropic = anthropic.Anthropic(api_key=anthropic_key) if anthropic_key else None\n",
    "        self.anthropic_ready = anthropic_key is not None\n",
    "        \n",
    "        print(\"\ud83d\udd0c Initialized clients:\")\n",
    "        print(f\"   {'\u2713' if self.minimax_ready else '\u2717'} MiniMax: {'Ready' if self.minimax_ready else 'No API key'}\")\n",
    "        print(f\"   {'\u2713' if self.openai_ready else '\u2717'} OpenAI: {'Ready' if self.openai_ready else 'No API key'}\")\n",
    "        print(f\"   {'\u2713' if self.anthropic_ready else '\u2717'} Anthropic: {'Ready' if self.anthropic_ready else 'No API key'}\")\n",
    "    \n",
    "    def complete_minimax(self, prompt: str, system: str, model: str = \"MiniMax-M2.1\", max_tokens: int = 16000) -> CompletionResult:\n",
    "        if not self.minimax_ready:\n",
    "            return CompletionResult(model_name=model, provider=\"MiniMax\", content=\"\", completion_time=0, \n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=\"Client not initialized\")\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            response = self.minimax.chat(messages, model=model, max_tokens=max_tokens, temperature=0.7)\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            return CompletionResult(\n",
    "                model_name=model, provider=\"MiniMax\",\n",
    "                content=response.choices[0].message.content,\n",
    "                completion_time=elapsed,\n",
    "                prompt_tokens=response.usage.prompt_tokens,\n",
    "                completion_tokens=response.usage.completion_tokens,\n",
    "                total_tokens=response.usage.total_tokens\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return CompletionResult(model_name=model, provider=\"MiniMax\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=str(e))\n",
    "    \n",
    "    def complete_openai(self, prompt: str, system: str, model: str = \"gpt-4o\", max_tokens: int = 16000) -> CompletionResult:\n",
    "        if not self.openai_ready:\n",
    "            return CompletionResult(model_name=model, provider=\"OpenAI\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=\"Set OPENAI_API_KEY\")\n",
    "        \n",
    "        is_o1 = model.startswith(\"o1\")\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"{system}\\n\\n{prompt}\"}] if is_o1 else [\n",
    "            {\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            kwargs = {\"model\": model, \"messages\": messages, \"max_completion_tokens\": max_tokens}\n",
    "            if not is_o1:\n",
    "                kwargs[\"temperature\"] = 0.7\n",
    "            response = self.openai.chat.completions.create(**kwargs)\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            return CompletionResult(\n",
    "                model_name=model, provider=\"OpenAI\",\n",
    "                content=response.choices[0].message.content,\n",
    "                completion_time=elapsed,\n",
    "                prompt_tokens=response.usage.prompt_tokens,\n",
    "                completion_tokens=response.usage.completion_tokens,\n",
    "                total_tokens=response.usage.total_tokens\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return CompletionResult(model_name=model, provider=\"OpenAI\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=str(e))\n",
    "    \n",
    "    def complete_anthropic(self, prompt: str, system: str, model: str = \"claude-sonnet-4-20250514\", max_tokens: int = 16000) -> CompletionResult:\n",
    "        if not self.anthropic_ready:\n",
    "            return CompletionResult(model_name=model, provider=\"Anthropic\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=\"Set ANTHROPIC_API_KEY\")\n",
    "        \n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            response = self.anthropic.messages.create(\n",
    "                model=model, max_tokens=max_tokens, system=system,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            return CompletionResult(\n",
    "                model_name=model, provider=\"Anthropic\",\n",
    "                content=response.content[0].text,\n",
    "                completion_time=elapsed,\n",
    "                prompt_tokens=response.usage.input_tokens,\n",
    "                completion_tokens=response.usage.output_tokens,\n",
    "                total_tokens=response.usage.input_tokens + response.usage.output_tokens\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return CompletionResult(model_name=model, provider=\"Anthropic\", content=\"\", completion_time=0,\n",
    "                                   prompt_tokens=0, completion_tokens=0, total_tokens=0, error=str(e))\n",
    "\n",
    "# Initialize client\n",
    "client = MultiModelClient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an elite full-stack developer specializing in Next.js 14+, React 18+, TypeScript, and complex interactive applications.\n",
    "\n",
    "Generate production-ready code following these standards:\n",
    "- Use Next.js App Router (app/ directory structure)\n",
    "- TypeScript with strict types - absolutely no 'any' types\n",
    "- React Server Components where appropriate\n",
    "- Client Components with 'use client' directive when needed\n",
    "- Tailwind CSS for all styling\n",
    "- Proper error handling and loading states\n",
    "- Accessible HTML (ARIA labels, semantic elements)\n",
    "- Performance optimized (useMemo, useCallback, React.memo where needed)\n",
    "- Custom hooks for reusable logic\n",
    "\n",
    "Output format: Provide complete file contents with clear file path comments.\n",
    "Use this format for each file:\n",
    "// === FILE: path/to/file.tsx ===\n",
    "<file contents>\n",
    "// === END FILE ===\n",
    "\n",
    "Generate ALL required files for a fully working application. Include package.json with all dependencies.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Create \"SYNTH CANVAS\" - A complete real-time collaborative infinite whiteboard with physics simulation and creative tools.\n",
    "\n",
    "## CORE FEATURES:\n",
    "\n",
    "### 1. Infinite Canvas System\n",
    "- Infinite pan/zoom canvas using HTML5 Canvas + React\n",
    "- Minimap showing viewport position on the canvas\n",
    "- Smooth zoom (0.1x to 10x) with mouse wheel + pinch gesture\n",
    "- Pan with middle mouse button or two-finger drag\n",
    "- Grid background that scales with zoom level\n",
    "- Viewport coordinates display\n",
    "\n",
    "### 2. Drawing Tools (implement ALL):\n",
    "a) **Pen Tool**: Pressure-sensitive strokes (simulate with speed), smoothing algorithm\n",
    "b) **Shape Tool**: Rectangle, Ellipse, Triangle, Line, Arrow - with shift for perfect shapes\n",
    "c) **Text Tool**: Click to add text, editable inline, font size/color options\n",
    "d) **Eraser**: Removes strokes it touches\n",
    "e) **Selection Tool**: Click to select, drag to move, resize handles on corners\n",
    "f) **Sticky Notes**: Colored notes that can be moved and edited\n",
    "g) **Connector Lines**: Lines that connect between objects and stay connected when moved\n",
    "\n",
    "### 3. Physics Simulation Mode (toggleable):\n",
    "- Objects can be given physics properties\n",
    "- Gravity affects objects when enabled\n",
    "- Objects collide with each other\n",
    "- Friction and bounce properties\n",
    "- \"Throw\" objects by dragging and releasing\n",
    "- Constraints/springs between objects\n",
    "- Use requestAnimationFrame for smooth 60fps physics\n",
    "\n",
    "### 4. Layer System:\n",
    "- Multiple layers support\n",
    "- Layer visibility toggle\n",
    "- Layer opacity control\n",
    "- Layer reordering (drag to reorder)\n",
    "- Lock layers\n",
    "- Layer groups\n",
    "\n",
    "### 5. Real-time Collaboration (simulated with local state):\n",
    "- Multiple cursor display (show 3 fake collaborators moving around)\n",
    "- User presence indicators\n",
    "- Object locking when someone is editing\n",
    "- Change highlighting (flash objects when modified)\n",
    "- User color coding\n",
    "\n",
    "### 6. Advanced Features:\n",
    "a) **Undo/Redo**: Full history with Ctrl+Z / Ctrl+Shift+Z (implement with command pattern)\n",
    "b) **Copy/Paste**: Ctrl+C/V with offset pasting\n",
    "c) **Keyboard Shortcuts**: \n",
    "   - V: Select, P: Pen, R: Rectangle, E: Ellipse, T: Text, N: Sticky Note\n",
    "   - Delete/Backspace: Remove selected\n",
    "   - Ctrl+A: Select all\n",
    "   - Ctrl+D: Duplicate\n",
    "d) **Export**: Export visible area as PNG\n",
    "e) **Auto-save**: Save canvas state to localStorage every 5 seconds\n",
    "\n",
    "### 7. UI Components Required:\n",
    "\n",
    "```\n",
    "app/\n",
    "\u251c\u2500\u2500 layout.tsx (main layout with dark theme)\n",
    "\u251c\u2500\u2500 page.tsx (canvas page)\n",
    "\u251c\u2500\u2500 globals.css\n",
    "components/\n",
    "\u251c\u2500\u2500 canvas/\n",
    "\u2502   \u251c\u2500\u2500 InfiniteCanvas.tsx (main canvas with pan/zoom)\n",
    "\u2502   \u251c\u2500\u2500 CanvasRenderer.tsx (renders all objects)\n",
    "\u2502   \u251c\u2500\u2500 Grid.tsx (background grid)\n",
    "\u2502   \u251c\u2500\u2500 Minimap.tsx (minimap component)\n",
    "\u2502   \u2514\u2500\u2500 SelectionBox.tsx (selection rectangle)\n",
    "\u251c\u2500\u2500 tools/\n",
    "\u2502   \u251c\u2500\u2500 Toolbar.tsx (floating toolbar)\n",
    "\u2502   \u251c\u2500\u2500 ToolButton.tsx\n",
    "\u2502   \u251c\u2500\u2500 ColorPicker.tsx\n",
    "\u2502   \u251c\u2500\u2500 StrokeWidthPicker.tsx\n",
    "\u2502   \u2514\u2500\u2500 LayerPanel.tsx\n",
    "\u251c\u2500\u2500 objects/\n",
    "\u2502   \u251c\u2500\u2500 StrokeObject.tsx\n",
    "\u2502   \u251c\u2500\u2500 ShapeObject.tsx\n",
    "\u2502   \u251c\u2500\u2500 TextObject.tsx\n",
    "\u2502   \u251c\u2500\u2500 StickyNote.tsx\n",
    "\u2502   \u2514\u2500\u2500 ConnectorLine.tsx\n",
    "\u251c\u2500\u2500 physics/\n",
    "\u2502   \u251c\u2500\u2500 PhysicsEngine.tsx\n",
    "\u2502   \u2514\u2500\u2500 PhysicsControls.tsx\n",
    "\u251c\u2500\u2500 collaboration/\n",
    "\u2502   \u251c\u2500\u2500 CursorOverlay.tsx\n",
    "\u2502   \u2514\u2500\u2500 PresenceIndicator.tsx\n",
    "\u2514\u2500\u2500 ui/\n",
    "    \u251c\u2500\u2500 Button.tsx\n",
    "    \u251c\u2500\u2500 Slider.tsx\n",
    "    \u251c\u2500\u2500 Popover.tsx\n",
    "    \u2514\u2500\u2500 Tooltip.tsx\n",
    "context/\n",
    "\u251c\u2500\u2500 CanvasContext.tsx (canvas state: objects, viewport, selection)\n",
    "\u251c\u2500\u2500 ToolContext.tsx (current tool, color, stroke width)\n",
    "\u251c\u2500\u2500 HistoryContext.tsx (undo/redo stack)\n",
    "\u2514\u2500\u2500 PhysicsContext.tsx (physics simulation state)\n",
    "hooks/\n",
    "\u251c\u2500\u2500 useCanvas.ts\n",
    "\u251c\u2500\u2500 usePanZoom.ts\n",
    "\u251c\u2500\u2500 useDrawing.ts\n",
    "\u251c\u2500\u2500 useSelection.ts\n",
    "\u251c\u2500\u2500 usePhysics.ts\n",
    "\u251c\u2500\u2500 useKeyboardShortcuts.ts\n",
    "\u2514\u2500\u2500 useHistory.ts\n",
    "lib/\n",
    "\u251c\u2500\u2500 canvas-utils.ts (coordinate transforms, hit testing)\n",
    "\u251c\u2500\u2500 physics-utils.ts (collision detection, physics math)\n",
    "\u251c\u2500\u2500 geometry.ts (shape calculations)\n",
    "\u2514\u2500\u2500 export.ts (PNG export)\n",
    "types/\n",
    "\u2514\u2500\u2500 index.ts\n",
    "```\n",
    "\n",
    "### 8. Type Definitions (types/index.ts):\n",
    "```typescript\n",
    "type Tool = 'select' | 'pen' | 'rectangle' | 'ellipse' | 'triangle' | 'line' | 'arrow' | 'text' | 'sticky' | 'connector' | 'eraser';\n",
    "\n",
    "interface Point { x: number; y: number; }\n",
    "interface Bounds { x: number; y: number; width: number; height: number; }\n",
    "\n",
    "interface CanvasObject {\n",
    "  id: string;\n",
    "  type: 'stroke' | 'shape' | 'text' | 'sticky' | 'connector';\n",
    "  layerId: string;\n",
    "  locked: boolean;\n",
    "  physics?: PhysicsBody;\n",
    "  createdAt: number;\n",
    "  updatedAt: number;\n",
    "}\n",
    "\n",
    "interface StrokeObject extends CanvasObject {\n",
    "  type: 'stroke';\n",
    "  points: Point[];\n",
    "  color: string;\n",
    "  width: number;\n",
    "  opacity: number;\n",
    "}\n",
    "\n",
    "interface ShapeObject extends CanvasObject {\n",
    "  type: 'shape';\n",
    "  shapeType: 'rectangle' | 'ellipse' | 'triangle' | 'line' | 'arrow';\n",
    "  bounds: Bounds;\n",
    "  color: string;\n",
    "  fill: string | null;\n",
    "  strokeWidth: number;\n",
    "  rotation: number;\n",
    "}\n",
    "\n",
    "interface TextObject extends CanvasObject {\n",
    "  type: 'text';\n",
    "  position: Point;\n",
    "  content: string;\n",
    "  fontSize: number;\n",
    "  fontFamily: string;\n",
    "  color: string;\n",
    "}\n",
    "\n",
    "interface StickyNote extends CanvasObject {\n",
    "  type: 'sticky';\n",
    "  bounds: Bounds;\n",
    "  content: string;\n",
    "  backgroundColor: string;\n",
    "}\n",
    "\n",
    "interface PhysicsBody {\n",
    "  velocity: Point;\n",
    "  acceleration: Point;\n",
    "  mass: number;\n",
    "  friction: number;\n",
    "  restitution: number;\n",
    "  isStatic: boolean;\n",
    "}\n",
    "\n",
    "interface Layer {\n",
    "  id: string;\n",
    "  name: string;\n",
    "  visible: boolean;\n",
    "  locked: boolean;\n",
    "  opacity: number;\n",
    "  order: number;\n",
    "}\n",
    "\n",
    "interface Viewport {\n",
    "  x: number;\n",
    "  y: number;\n",
    "  zoom: number;\n",
    "}\n",
    "\n",
    "interface HistoryEntry {\n",
    "  type: 'add' | 'remove' | 'modify' | 'move';\n",
    "  objects: CanvasObject[];\n",
    "  previousState?: CanvasObject[];\n",
    "}\n",
    "```\n",
    "\n",
    "### 9. Styling Requirements:\n",
    "- Dark theme: bg-slate-950, panels bg-slate-900/80 with backdrop-blur\n",
    "- Accent colors: violet-500 for primary, cyan-400 for secondary\n",
    "- Floating toolbar with glassmorphism effect\n",
    "- Smooth transitions (150ms) on all interactions\n",
    "- Tool icons using SVG (create simple geometric icons)\n",
    "- Cursor changes based on current tool\n",
    "- Selection handles with corner resize cursors\n",
    "- Neon glow effect on selected objects\n",
    "\n",
    "### 10. Performance Requirements:\n",
    "- Canvas should handle 1000+ objects smoothly\n",
    "- Use object spatial indexing for hit testing\n",
    "- Throttle canvas redraws\n",
    "- Only render objects in viewport\n",
    "- Use OffscreenCanvas for complex operations if supported\n",
    "\n",
    "Generate a complete, fully functional application. This should be an impressive demo of complex canvas manipulation, state management, and physics simulation.\"\"\"\n",
    "\n",
    "print(\"\ud83d\udcdd Prompt ready!\")\n",
    "print(f\"   System prompt: {len(SYSTEM_PROMPT):,} chars\")\n",
    "print(f\"   User prompt: {len(USER_PROMPT):,} chars\")\n",
    "print(f\"   Total: {len(SYSTEM_PROMPT) + len(USER_PROMPT):,} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T21:58:48.306129Z",
     "iopub.status.busy": "2025-12-30T21:58:48.306037Z",
     "iopub.status.idle": "2025-12-30T21:58:48.308331Z",
     "shell.execute_reply": "2025-12-30T21:58:48.307939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udccc Models selected for comparison:\n",
      "\n",
      "  MINIMAX:\n",
      "    \u2022 MiniMax-M2.1\n",
      "\n",
      "  OPENAI:\n",
      "    \u2022 gpt-4o\n",
      "\n",
      "  Total: 2 models\n"
     ]
    }
   ],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \ud83c\udf9b\ufe0f CONFIGURATION: Select which models to compare\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "MODELS_TO_COMPARE = {\n",
    "    # MiniMax\n",
    "    \"minimax\": [\n",
    "        \"MiniMax-M2.1\",\n",
    "    ],\n",
    "    \n",
    "    # OpenAI - uncomment models you want to test\n",
    "    \"openai\": [\n",
    "        \"gpt-4o\",\n",
    "        # \"gpt-4o-mini\",        # Faster, cheaper\n",
    "        # \"gpt-4-turbo\",        # Previous flagship\n",
    "        # \"o1\",                 # Reasoning model (very slow for code gen)\n",
    "    ],\n",
    "    \n",
    "    # Anthropic - no free tier, uncomment if you have API access\n",
    "    \"anthropic\": [\n",
    "        # \"claude-sonnet-4-20250514\",   # Latest Sonnet\n",
    "        \"claude-3-5-sonnet-20241022\", # Previous Sonnet\n",
    "        # \"claude-3-opus-20240229\",     # Most capable (expensive)\n",
    "        # \"claude-3-5-haiku-20241022\",  # Fast and cheap\n",
    "    ],\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udccc Models selected for comparison:\")\n",
    "total = 0\n",
    "for provider, models in MODELS_TO_COMPARE.items():\n",
    "    if models:\n",
    "        print(f\"\\n  {provider.upper()}:\")\n",
    "        for model in models:\n",
    "            print(f\"    \u2022 {model}\")\n",
    "            total += 1\n",
    "print(f\"\\n  Total: {total} models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Next.js Application Prompt\n",
    "\n",
    "A complex Next.js 14 application with:\n",
    "- TypeScript throughout\n",
    "- App Router architecture\n",
    "- Multiple reusable components\n",
    "- State management with Context\n",
    "- Tailwind CSS styling\n",
    "- Kanban board with drag & drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T21:58:48.309317Z",
     "iopub.status.busy": "2025-12-30T21:58:48.309241Z",
     "iopub.status.idle": "2025-12-30T21:58:48.311531Z",
     "shell.execute_reply": "2025-12-30T21:58:48.311206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcdd Prompt ready!\n",
      "   System prompt: 781 chars\n",
      "   User prompt: 2,385 chars\n",
      "   Total: 3,166 chars\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a senior full-stack developer specializing in Next.js 14+, React 18+, and TypeScript.\n",
    "\n",
    "Generate production-ready code following these standards:\n",
    "- Use Next.js App Router (app/ directory structure)\n",
    "- TypeScript with strict types - no 'any' types\n",
    "- React Server Components where appropriate\n",
    "- Client Components with 'use client' directive when needed\n",
    "- Tailwind CSS for all styling\n",
    "- Proper error handling and loading states\n",
    "- Accessible HTML (ARIA labels, semantic elements)\n",
    "- Performance optimized (lazy loading, memoization where needed)\n",
    "\n",
    "Output format: Provide complete file contents with clear file path comments.\n",
    "Use this format for each file:\n",
    "// === FILE: path/to/file.tsx ===\n",
    "<file contents>\n",
    "// === END FILE ===\n",
    "\n",
    "Generate ALL required files for a working application.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Create a complete Next.js 14 application: \"TaskFlow\" - A modern task management dashboard.\n",
    "\n",
    "## Required Features:\n",
    "\n",
    "### 1. Layout & Navigation (app/layout.tsx)\n",
    "- Dark theme with gradient accents (slate-900 to slate-800)\n",
    "- Responsive sidebar navigation with icons\n",
    "- Collapsible sidebar on mobile\n",
    "- Header with search bar and user avatar dropdown\n",
    "- Breadcrumb navigation\n",
    "\n",
    "### 2. Dashboard Page (app/page.tsx)\n",
    "- Overview cards: Total Tasks, Completed, In Progress, Overdue (with animated counters)\n",
    "- Task completion chart (simple CSS-based bar chart)\n",
    "- Recent activity feed with timestamps\n",
    "- Quick-add task floating button\n",
    "\n",
    "### 3. Task Board Page (app/tasks/page.tsx)\n",
    "- Kanban-style board with 4 columns: Backlog, To Do, In Progress, Done\n",
    "- Drag and drop tasks between columns (implement with React state, no external libs)\n",
    "- Task cards showing: title, priority badge, due date, assignee avatar\n",
    "- Filter by priority and search\n",
    "- Add new task modal\n",
    "\n",
    "### 4. Components Required:\n",
    "- TaskCard.tsx - Individual task with hover effects, priority colors\n",
    "- KanbanColumn.tsx - Column with task count and drop zone styling\n",
    "- Modal.tsx - Reusable modal with animations\n",
    "- Button.tsx - Styled button with variants (primary, secondary, danger)\n",
    "- Badge.tsx - Priority/status badges\n",
    "- Avatar.tsx - User avatar with fallback initials\n",
    "- SearchInput.tsx - Animated search with icon\n",
    "- Sidebar.tsx - Navigation with active states\n",
    "\n",
    "### 5. State Management:\n",
    "- Use React Context for task state (TaskContext.tsx)\n",
    "- Custom hooks: useTask, useDragDrop\n",
    "- Persist to localStorage\n",
    "\n",
    "### 6. Types (types/index.ts):\n",
    "```typescript\n",
    "interface Task {\n",
    "  id: string;\n",
    "  title: string;\n",
    "  description?: string;\n",
    "  status: 'backlog' | 'todo' | 'in-progress' | 'done';\n",
    "  priority: 'low' | 'medium' | 'high' | 'urgent';\n",
    "  dueDate?: string;\n",
    "  assignee?: User;\n",
    "  createdAt: string;\n",
    "}\n",
    "\n",
    "interface User {\n",
    "  id: string;\n",
    "  name: string;\n",
    "  avatar?: string;\n",
    "}\n",
    "```\n",
    "\n",
    "### 7. Styling Requirements:\n",
    "- Glassmorphism cards with backdrop-blur\n",
    "- Smooth transitions on all interactive elements (300ms)\n",
    "- Gradient borders on focused inputs\n",
    "- Hover lift effect on cards (transform + shadow)\n",
    "- Loading skeletons for async content\n",
    "- Toast notifications for actions\n",
    "\n",
    "### 8. Sample Data:\n",
    "Include realistic sample tasks (8-10 tasks across all columns) with varied priorities and due dates.\n",
    "\n",
    "Generate a complete, working Next.js application with all files needed.\"\"\"\n",
    "\n",
    "print(\"\ud83d\udcdd Prompt ready!\")\n",
    "print(f\"   System prompt: {len(SYSTEM_PROMPT):,} chars\")\n",
    "print(f\"   User prompt: {len(USER_PROMPT):,} chars\")\n",
    "print(f\"   Total: {len(SYSTEM_PROMPT) + len(USER_PROMPT):,} chars\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T21:58:48.312560Z",
     "iopub.status.busy": "2025-12-30T21:58:48.312488Z",
     "iopub.status.idle": "2025-12-30T22:02:07.906720Z",
     "shell.execute_reply": "2025-12-30T22:02:07.905129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\ude80 Starting Next.js application generation comparison...\n",
      "======================================================================\n",
      "\u26a0\ufe0f  This is a complex prompt - expect 30-120 seconds per model!\n",
      "\n",
      "\n",
      "\u23f3 Running MiniMax: MiniMax-M2.1...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
      "\u2551  MiniMax: MiniMax-M2.1\n",
      "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n",
      "\u2551  \u23f1\ufe0f  Completion Time:     143.34s\n",
      "\u2551  \ud83d\udcdd Prompt Tokens:        744\n",
      "\u2551  \u270d\ufe0f  Completion Tokens:   16,000\n",
      "\u2551  \ud83d\udcca Total Tokens:         16,744\n",
      "\u2551  \u26a1 Speed:                111.6 tokens/sec\n",
      "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
      "\n",
      "\n",
      "\u23f3 Running OpenAI: gpt-4o...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
      "\u2551  OpenAI: gpt-4o\n",
      "\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n",
      "\u2551  \u23f1\ufe0f  Completion Time:     56.24s\n",
      "\u2551  \ud83d\udcdd Prompt Tokens:        742\n",
      "\u2551  \u270d\ufe0f  Completion Tokens:   2,620\n",
      "\u2551  \ud83d\udcca Total Tokens:         3,362\n",
      "\u2551  \u26a1 Speed:                46.6 tokens/sec\n",
      "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\u2705 Completed 2 model comparisons!\n"
     ]
    }
   ],
   "source": [
    "# Run the comparison\n",
    "results: dict[str, CompletionResult] = {}\n",
    "\n",
    "print(\"\ud83d\ude80 Starting Next.js application generation comparison...\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\u26a0\ufe0f  This is a complex prompt - expect 30-120 seconds per model!\\n\")\n",
    "\n",
    "# MiniMax\n",
    "for model in MODELS_TO_COMPARE.get(\"minimax\", []):\n",
    "    print(f\"\\n\u23f3 Running MiniMax: {model}...\")\n",
    "    result = client.complete_minimax(USER_PROMPT, SYSTEM_PROMPT, model=model)\n",
    "    results[f\"minimax_{model}\"] = result\n",
    "    print(result.summary())\n",
    "\n",
    "# OpenAI\n",
    "for model in MODELS_TO_COMPARE.get(\"openai\", []):\n",
    "    print(f\"\\n\u23f3 Running OpenAI: {model}...\")\n",
    "    result = client.complete_openai(USER_PROMPT, SYSTEM_PROMPT, model=model)\n",
    "    results[f\"openai_{model}\"] = result\n",
    "    print(result.summary())\n",
    "\n",
    "# Anthropic\n",
    "for model in MODELS_TO_COMPARE.get(\"anthropic\", []):\n",
    "    print(f\"\\n\u23f3 Running Anthropic: {model}...\")\n",
    "    result = client.complete_anthropic(USER_PROMPT, SYSTEM_PROMPT, model=model)\n",
    "    results[f\"anthropic_{model}\"] = result\n",
    "    print(result.summary())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"\u2705 Completed {len(results)} model comparisons!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:02:07.912460Z",
     "iopub.status.busy": "2025-12-30T22:02:07.912228Z",
     "iopub.status.idle": "2025-12-30T22:02:07.932838Z",
     "shell.execute_reply": "2025-12-30T22:02:07.932375Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## \ud83d\udcca Comparison Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Provider</th>\n",
       "      <th>Model</th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Speed (tok/s)</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MiniMax</td>\n",
       "      <td>MiniMax-M2.1</td>\n",
       "      <td>143.34</td>\n",
       "      <td>16,000</td>\n",
       "      <td>111.6</td>\n",
       "      <td>54,049 chars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenAI</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>56.24</td>\n",
       "      <td>2,620</td>\n",
       "      <td>46.6</td>\n",
       "      <td>10,437 chars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Provider         Model Time (s)  Tokens Speed (tok/s)        Output\n",
       "0  MiniMax  MiniMax-M2.1   143.34  16,000         111.6  54,049 chars\n",
       "1   OpenAI        gpt-4o    56.24   2,620          46.6  10,437 chars"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83c\udfc6 Results:\n",
      "   \u26a1 Fastest: OpenAI gpt-4o (56.24s)\n",
      "   \ud83d\ude80 Highest speed: MiniMax MiniMax-M2.1 (111.6 tok/s)\n",
      "   \ud83d\udcdd Most code: MiniMax MiniMax-M2.1 (16,000 tokens)\n"
     ]
    }
   ],
   "source": [
    "# Comparison summary\n",
    "successful = {k: v for k, v in results.items() if v.success}\n",
    "\n",
    "if successful:\n",
    "    display(Markdown(\"## \ud83d\udcca Comparison Summary\"))\n",
    "    \n",
    "    if HAS_PANDAS:\n",
    "        data = [{\n",
    "            'Provider': r.provider,\n",
    "            'Model': r.model_name,\n",
    "            'Time (s)': f\"{r.completion_time:.2f}\",\n",
    "            'Tokens': f\"{r.completion_tokens:,}\",\n",
    "            'Speed (tok/s)': f\"{r.tokens_per_second:.1f}\",\n",
    "            'Output': f\"{len(r.content):,} chars\"\n",
    "        } for r in successful.values()]\n",
    "        display(pd.DataFrame(data))\n",
    "    else:\n",
    "        print(f\"{'Provider':<12} {'Model':<28} {'Time':<10} {'Tokens':<10} {'Speed':<12}\")\n",
    "        print(\"-\" * 75)\n",
    "        for r in successful.values():\n",
    "            print(f\"{r.provider:<12} {r.model_name:<28} {r.completion_time:.2f}s     {r.completion_tokens:<10,} {r.tokens_per_second:.1f} tok/s\")\n",
    "    \n",
    "    # Winners\n",
    "    fastest = min(successful.values(), key=lambda x: x.completion_time)\n",
    "    most_output = max(successful.values(), key=lambda x: x.completion_tokens)\n",
    "    highest_speed = max(successful.values(), key=lambda x: x.tokens_per_second)\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfc6 Results:\")\n",
    "    print(f\"   \u26a1 Fastest: {fastest.provider} {fastest.model_name} ({fastest.completion_time:.2f}s)\")\n",
    "    print(f\"   \ud83d\ude80 Highest speed: {highest_speed.provider} {highest_speed.model_name} ({highest_speed.tokens_per_second:.1f} tok/s)\")\n",
    "    print(f\"   \ud83d\udcdd Most code: {most_output.provider} {most_output.model_name} ({most_output.completion_tokens:,} tokens)\")\n",
    "else:\n",
    "    print(\"\u274c No successful completions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:02:07.934201Z",
     "iopub.status.busy": "2025-12-30T22:02:07.934084Z",
     "iopub.status.idle": "2025-12-30T22:02:07.940340Z",
     "shell.execute_reply": "2025-12-30T22:02:07.939907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## \ud83d\udd0d Code Quality Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MiniMax - MiniMax-M2.1\n",
      "-------------------------------------------------------\n",
      "  Lines               : 1680\n",
      "  Characters          : 52630\n",
      "  Files Found         : 18\n",
      "  TypeScript          : \u2713\n",
      "  App Router          : \u2713\n",
      "  Client Components   : \u2713\n",
      "  React Hooks         : \u2713\n",
      "  Custom Hooks        : \u2713\n",
      "  Tailwind            : \u2713\n",
      "  Context API         : \u2713\n",
      "  Type Definitions    : \u2713\n",
      "  Animations          : \u2713\n",
      "  Accessibility       : \u2717\n",
      "\n",
      "OpenAI - gpt-4o\n",
      "-------------------------------------------------------\n",
      "  Lines               : 394\n",
      "  Characters          : 10437\n",
      "  Files Found         : 19\n",
      "  TypeScript          : \u2713\n",
      "  App Router          : \u2713\n",
      "  Client Components   : \u2713\n",
      "  React Hooks         : \u2713\n",
      "  Custom Hooks        : \u2713\n",
      "  Tailwind            : \u2713\n",
      "  Context API         : \u2713\n",
      "  Type Definitions    : \u2713\n",
      "  Animations          : \u2713\n",
      "  Accessibility       : \u2713\n"
     ]
    }
   ],
   "source": [
    "# Code quality analysis\n",
    "def analyze_code(content: str) -> dict:\n",
    "    \"\"\"Analyze Next.js code output for completeness.\"\"\"\n",
    "    content = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)\n",
    "    lower = content.lower()\n",
    "    \n",
    "    files = re.findall(r'(?:// ===|/\\*\\*|###|File:)\\s*(?:FILE:?)?\\s*([\\w./\\-]+\\.(?:tsx?|js|css|json))', content, re.I)\n",
    "    \n",
    "    return {\n",
    "        'Lines': len(content.split('\\n')),\n",
    "        'Characters': len(content),\n",
    "        'Files Found': len(set(files)) if files else '?',\n",
    "        'TypeScript': '\u2713' if ('.tsx' in content or 'interface ' in content) else '\u2717',\n",
    "        'App Router': '\u2713' if ('app/' in content) else '\u2717',\n",
    "        'Client Components': '\u2713' if (\"'use client'\" in content) else '\u2717',\n",
    "        'React Hooks': '\u2713' if any(h in lower for h in ['usestate', 'useeffect', 'usecontext']) else '\u2717',\n",
    "        'Custom Hooks': '\u2713' if ('function use' in lower or 'const use' in lower) else '\u2717',\n",
    "        'Tailwind': '\u2713' if any(c in content for c in ['className=', 'bg-', 'flex ', 'grid ']) else '\u2717',\n",
    "        'Context API': '\u2713' if 'createcontext' in lower else '\u2717',\n",
    "        'Type Definitions': '\u2713' if ('interface ' in content or 'type ' in content) else '\u2717',\n",
    "        'Animations': '\u2713' if ('transition' in lower or 'animate' in lower) else '\u2717',\n",
    "        'Accessibility': '\u2713' if ('aria-' in lower or 'role=' in lower) else '\u2717',\n",
    "    }\n",
    "\n",
    "display(Markdown(\"## \ud83d\udd0d Code Quality Analysis\"))\n",
    "\n",
    "for name, result in results.items():\n",
    "    if result.success:\n",
    "        print(f\"\\n{result.provider} - {result.model_name}\")\n",
    "        print(\"-\" * 55)\n",
    "        analysis = analyze_code(result.content)\n",
    "        for metric, value in analysis.items():\n",
    "            print(f\"  {metric:<20}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:02:07.941539Z",
     "iopub.status.busy": "2025-12-30T22:02:07.941460Z",
     "iopub.status.idle": "2025-12-30T22:02:07.945363Z",
     "shell.execute_reply": "2025-12-30T22:02:07.945080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## \ud83d\udcc8 Visual Comparison"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u23f1\ufe0f  Completion Time (lower = better)\n",
      "===========================================================================\n",
      "MiniMax MiniMax-M2.1 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 143.34s\n",
      "OpenAI gpt-4o        \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 56.24s\n",
      "\n",
      "\n",
      "\u26a1 Generation Speed (higher = better)\n",
      "===========================================================================\n",
      "MiniMax MiniMax-M2.1 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 111.62 tok/s\n",
      "OpenAI gpt-4o        \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 46.58 tok/s\n",
      "\n",
      "\n",
      "\ud83d\udcdd Output Size\n",
      "===========================================================================\n",
      "MiniMax MiniMax-M2.1 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 54.05 KB\n",
      "OpenAI gpt-4o        \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 10.44 KB\n",
      "\n",
      "\n",
      "\ud83d\udd22 Tokens Generated\n",
      "===========================================================================\n",
      "MiniMax MiniMax-M2.1 \u2502 \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 16000.00\n",
      "OpenAI gpt-4o        \u2502 \u2588\u2588\u2588\u2588\u2588\u2588 2620.00\n"
     ]
    }
   ],
   "source": [
    "# Visual comparison charts\n",
    "def bar_chart(data: dict, title: str, unit: str = \"\", width: int = 40):\n",
    "    \"\"\"Simple ASCII bar chart.\"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    max_val = max(data.values())\n",
    "    max_label = max(len(k) for k in data.keys())\n",
    "    \n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * (max_label + width + 15))\n",
    "    for label, val in sorted(data.items(), key=lambda x: -x[1]):\n",
    "        bar = \"\u2588\" * int((val / max_val) * width) if max_val > 0 else \"\"\n",
    "        print(f\"{label:<{max_label}} \u2502 {bar} {val:.2f}{unit}\")\n",
    "\n",
    "if successful:\n",
    "    display(Markdown(\"## \ud83d\udcc8 Visual Comparison\"))\n",
    "    \n",
    "    bar_chart({f\"{r.provider} {r.model_name[:18]}\": r.completion_time for r in successful.values()},\n",
    "              \"\u23f1\ufe0f  Completion Time (lower = better)\", \"s\")\n",
    "    \n",
    "    bar_chart({f\"{r.provider} {r.model_name[:18]}\": r.tokens_per_second for r in successful.values()},\n",
    "              \"\\n\u26a1 Generation Speed (higher = better)\", \" tok/s\")\n",
    "    \n",
    "    bar_chart({f\"{r.provider} {r.model_name[:18]}\": len(r.content)/1000 for r in successful.values()},\n",
    "              \"\\n\ud83d\udcdd Output Size\", \" KB\")\n",
    "    \n",
    "    bar_chart({f\"{r.provider} {r.model_name[:18]}\": r.completion_tokens for r in successful.values()},\n",
    "              \"\\n\ud83d\udd22 Tokens Generated\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:02:07.946828Z",
     "iopub.status.busy": "2025-12-30T22:02:07.946732Z",
     "iopub.status.idle": "2025-12-30T22:02:07.953675Z",
     "shell.execute_reply": "2025-12-30T22:02:07.953227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## \ud83d\udcbe Save Generated Code"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 generated_nextjs/minimax_MiniMax_M2_1.md\n",
      "\u2705 generated_nextjs/openai_gpt_4o.md\n",
      "\n",
      "\ud83d\udcc1 Saved 2 files to 'generated_nextjs/'\n",
      "   Open these files to compare the generated code!\n"
     ]
    }
   ],
   "source": [
    "# Save generated code to files\n",
    "output_dir = \"generated_nextjs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "display(Markdown(\"## \ud83d\udcbe Save Generated Code\"))\n",
    "\n",
    "saved = []\n",
    "for name, result in results.items():\n",
    "    if result.success:\n",
    "        safe = result.model_name.replace('.', '_').replace('-', '_').replace(' ', '_')\n",
    "        filename = f\"{output_dir}/{result.provider.lower()}_{safe}.md\"\n",
    "        \n",
    "        content = re.sub(r'<think>.*?</think>', '', result.content, flags=re.DOTALL).strip()\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(f\"# TaskFlow - Generated by {result.provider} {result.model_name}\\n\\n\")\n",
    "            f.write(f\"**Time:** {result.completion_time:.2f}s | **Tokens:** {result.completion_tokens:,}\\n\\n---\\n\\n\")\n",
    "            f.write(content)\n",
    "        \n",
    "        saved.append(filename)\n",
    "        print(f\"\u2705 {filename}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc1 Saved {len(saved)} files to '{output_dir}/'\")\n",
    "print(\"   Open these files to compare the generated code!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T22:02:07.954838Z",
     "iopub.status.busy": "2025-12-30T22:02:07.954740Z",
     "iopub.status.idle": "2025-12-30T22:02:07.960950Z",
     "shell.execute_reply": "2025-12-30T22:02:07.960553Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Results saved to benchmark_results/05_nextjs_comparison.json\n",
      "\n",
      "\ud83d\udcca Summary:\n",
      "   Models compared: 2\n",
      "   Providers: OpenAI, MiniMax\n",
      "\n",
      "\ud83c\udfc6 Winners:\n",
      "   fastest: OpenAI gpt-4o\n",
      "   most_output: MiniMax MiniMax-M2.1\n",
      "   highest_throughput: MiniMax MiniMax-M2.1\n",
      "   most_files: OpenAI gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Save benchmark results\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "os.makedirs(\"benchmark_results\", exist_ok=True)\n",
    "\n",
    "# Build results from the comparison\n",
    "successful = {k: v for k, v in results.items() if v.success}\n",
    "\n",
    "model_results = []\n",
    "for name, result in successful.items():\n",
    "    # Get code analysis metrics\n",
    "    analysis = analyze_code(result.content)\n",
    "    \n",
    "    model_results.append({\n",
    "        \"key\": name,\n",
    "        \"provider\": result.provider,\n",
    "        \"model\": result.model_name,\n",
    "        \"completion_time\": round(result.completion_time, 2),\n",
    "        \"prompt_tokens\": result.prompt_tokens,\n",
    "        \"completion_tokens\": result.completion_tokens,\n",
    "        \"total_tokens\": result.total_tokens,\n",
    "        \"tokens_per_second\": round(result.tokens_per_second, 1),\n",
    "        \"output_chars\": len(result.content),\n",
    "        \"code_analysis\": {\n",
    "            \"lines\": analysis.get(\"Lines\", 0),\n",
    "            \"files_found\": analysis.get(\"Files Found\", 0),\n",
    "            \"has_typescript\": analysis.get(\"TypeScript\") == \"\u2713\",\n",
    "            \"has_app_router\": analysis.get(\"App Router\") == \"\u2713\",\n",
    "            \"has_react_hooks\": analysis.get(\"React Hooks\") == \"\u2713\",\n",
    "            \"has_tailwind\": analysis.get(\"Tailwind\") == \"\u2713\",\n",
    "            \"has_context_api\": analysis.get(\"Context API\") == \"\u2713\",\n",
    "            \"has_type_definitions\": analysis.get(\"Type Definitions\") == \"\u2713\",\n",
    "            \"has_animations\": analysis.get(\"Animations\") == \"\u2713\",\n",
    "            \"has_accessibility\": analysis.get(\"Accessibility\") == \"\u2713\"\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Calculate winners\n",
    "if successful:\n",
    "    fastest = min(successful.values(), key=lambda x: x.completion_time)\n",
    "    most_output = max(successful.values(), key=lambda x: x.completion_tokens)\n",
    "    highest_speed = max(successful.values(), key=lambda x: x.tokens_per_second)\n",
    "    most_files = max(model_results, key=lambda x: x[\"code_analysis\"][\"files_found\"])\n",
    "    \n",
    "    winners = {\n",
    "        \"fastest\": f\"{fastest.provider} {fastest.model_name}\",\n",
    "        \"most_output\": f\"{most_output.provider} {most_output.model_name}\",\n",
    "        \"highest_throughput\": f\"{highest_speed.provider} {highest_speed.model_name}\",\n",
    "        \"most_files\": f\"{most_files['provider']} {most_files['model']}\"\n",
    "    }\n",
    "else:\n",
    "    winners = {}\n",
    "\n",
    "# Build final results\n",
    "benchmark_results = {\n",
    "    \"notebook\": \"05_nextjs_comparison\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"task\": \"nextjs_application_generation\",\n",
    "    \"summary\": {\n",
    "        \"models_compared\": len(model_results),\n",
    "        \"providers\": list(set(r[\"provider\"] for r in model_results)),\n",
    "        \"winners\": winners\n",
    "    },\n",
    "    \"models\": model_results,\n",
    "    \"minimax_performance\": next((r for r in model_results if r[\"provider\"] == \"MiniMax\"), None)\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_path = \"benchmark_results/05_nextjs_comparison.json\"\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(benchmark_results, f, indent=2)\n",
    "\n",
    "print(f\"\u2705 Results saved to {output_path}\")\n",
    "print(f\"\\n\ud83d\udcca Summary:\")\n",
    "print(f\"   Models compared: {benchmark_results['summary']['models_compared']}\")\n",
    "print(f\"   Providers: {', '.join(benchmark_results['summary']['providers'])}\")\n",
    "if winners:\n",
    "    print(f\"\\n\ud83c\udfc6 Winners:\")\n",
    "    for category, winner in winners.items():\n",
    "        print(f\"   {category}: {winner}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}